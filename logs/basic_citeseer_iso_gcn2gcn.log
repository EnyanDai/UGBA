nohup: ignoring input
Namespace(clean_test_nodes_num=200, cuda=True, dataset='citeseer', debug=True, defense_mode='isolate', device_id=5, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.6, homo_loss_weight=0, load_benign_model=True, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.1, seed=10, target_class=0, target_test_nodes_num=200, test_model='GCN', thrd=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)
Length of training set: 480
=== training gcn model ===
Epoch 0, training loss: 1.7960103750228882
acc_val: 0.5950
Epoch 10, training loss: 0.24076996743679047
acc_val: 0.7175
Epoch 20, training loss: 0.07505097985267639
acc_val: 0.7075
Epoch 30, training loss: 0.042961616069078445
acc_val: 0.7100
Epoch 40, training loss: 0.03865412250161171
acc_val: 0.7100
Epoch 50, training loss: 0.049794893711805344
acc_val: 0.7075
Epoch 60, training loss: 0.04481234773993492
acc_val: 0.7050
Epoch 70, training loss: 0.04562455788254738
acc_val: 0.7075
Epoch 80, training loss: 0.03736679255962372
acc_val: 0.7100
Epoch 90, training loss: 0.03881681710481644
acc_val: 0.7125
Epoch 100, training loss: 0.03580740839242935
acc_val: 0.7100
Epoch 110, training loss: 0.03034024126827717
acc_val: 0.7150
Epoch 120, training loss: 0.03434022143483162
acc_val: 0.7075
Epoch 130, training loss: 0.03938857093453407
acc_val: 0.7050
Epoch 140, training loss: 0.03171335905790329
acc_val: 0.7000
Epoch 150, training loss: 0.028188984841108322
acc_val: 0.7000
Epoch 160, training loss: 0.027446221560239792
acc_val: 0.6975
Epoch 170, training loss: 0.026743894442915916
acc_val: 0.7050
Epoch 180, training loss: 0.029039038345217705
acc_val: 0.6900
Epoch 190, training loss: 0.031269557774066925
acc_val: 0.7000
=== picking the best model according to the performance on validation ===
Training benign model Finished!
Total time elapsed: 4.6465s
Benign model saved at ./modelpath/GCN_citeseer_benign.pth
Benign CA: 0.7100
Benign CA on clean test nodes: 0.6450
3327
200
Epoch 0, training loss: 1.790500283241272
acc_train_clean: 0.2146, acc_train_attach: 0.1304
Epoch 10, training loss: 0.26244547963142395
acc_train_clean: 0.9500, acc_train_attach: 0.9565
Epoch 20, training loss: 0.0915336012840271
acc_train_clean: 0.9792, acc_train_attach: 0.9565
Epoch 30, training loss: 0.06203740835189819
acc_train_clean: 0.9833, acc_train_attach: 1.0000
Epoch 40, training loss: 0.04943143576383591
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 50, training loss: 0.057540714740753174
acc_train_clean: 0.9833, acc_train_attach: 0.9565
Epoch 60, training loss: 0.06500132381916046
acc_train_clean: 0.9917, acc_train_attach: 0.9130
Epoch 70, training loss: 0.04825390502810478
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 80, training loss: 0.049989961087703705
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 90, training loss: 0.04556676745414734
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 100, training loss: 0.04579402133822441
acc_train_clean: 0.9875, acc_train_attach: 1.0000
Epoch 110, training loss: 0.04852156713604927
acc_train_clean: 0.9854, acc_train_attach: 1.0000
Epoch 120, training loss: 0.04452419653534889
acc_train_clean: 0.9854, acc_train_attach: 1.0000
Epoch 130, training loss: 0.03549171984195709
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 140, training loss: 0.04323604702949524
acc_train_clean: 0.9833, acc_train_attach: 1.0000
Epoch 150, training loss: 0.035920023918151855
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 160, training loss: 0.043722059577703476
acc_train_clean: 0.9917, acc_train_attach: 0.9565
Epoch 170, training loss: 0.03962855041027069
acc_train_clean: 0.9854, acc_train_attach: 1.0000
Epoch 180, training loss: 0.03461286425590515
acc_train_clean: 0.9938, acc_train_attach: 1.0000
Epoch 190, training loss: 0.03771847486495972
acc_train_clean: 0.9875, acc_train_attach: 1.0000
tensor([[   0,  628],
        [   7, 2137],
        [  10, 1670],
        ...,
        [3387, 1152],
        [3390, 1945],
        [3393, 1366]])
503
335
4454 3128
set()
=== training gcn model ===
Epoch 0, training loss: 1.79793119430542
acc_val: 0.4175
Epoch 10, training loss: 0.15592972934246063
acc_val: 0.6700
Epoch 20, training loss: 0.059208206832408905
acc_val: 0.6425
Epoch 30, training loss: 0.05692340061068535
acc_val: 0.6425
Epoch 40, training loss: 0.03431892395019531
acc_val: 0.6650
Epoch 50, training loss: 0.0386960506439209
acc_val: 0.6625
Epoch 60, training loss: 0.04776319861412048
acc_val: 0.6525
Epoch 70, training loss: 0.03385017067193985
acc_val: 0.6575
Epoch 80, training loss: 0.04534211754798889
acc_val: 0.6550
Epoch 90, training loss: 0.0388319157063961
acc_val: 0.6475
Epoch 100, training loss: 0.03848724812269211
acc_val: 0.6600
Epoch 110, training loss: 0.03553754836320877
acc_val: 0.6525
Epoch 120, training loss: 0.03853128105401993
acc_val: 0.6500
Epoch 130, training loss: 0.04082714021205902
acc_val: 0.6475
Epoch 140, training loss: 0.031134186312556267
acc_val: 0.6450
Epoch 150, training loss: 0.030728323385119438
acc_val: 0.6625
Epoch 160, training loss: 0.035959817469120026
acc_val: 0.6475
Epoch 170, training loss: 0.03424958884716034
acc_val: 0.6475
Epoch 180, training loss: 0.03630371764302254
acc_val: 0.6575
Epoch 190, training loss: 0.028521887958049774
acc_val: 0.6600
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.2174
accuracy on clean test nodes: 0.6650
ASR: 0.0250
CA: 0.6250
