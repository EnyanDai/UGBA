nohup: ignoring input
Namespace(clean_test_nodes_num=200, cuda=True, dataset='citeseer', debug=True, defense_mode='none', device_id=5, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.6, homo_loss_weight=0, load_benign_model=True, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.1, seed=10, target_class=0, target_test_nodes_num=200, test_model='GCN', thrd=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)
Length of training set: 480
=== training gcn model ===
Epoch 0, training loss: 1.7960103750228882
acc_val: 0.5950
Epoch 10, training loss: 0.24076993763446808
acc_val: 0.7175
Epoch 20, training loss: 0.07505097985267639
acc_val: 0.7075
Epoch 30, training loss: 0.04296162351965904
acc_val: 0.7100
Epoch 40, training loss: 0.03865412250161171
acc_val: 0.7100
Epoch 50, training loss: 0.04979485645890236
acc_val: 0.7075
Epoch 60, training loss: 0.044812120497226715
acc_val: 0.7050
Epoch 70, training loss: 0.04562867060303688
acc_val: 0.7075
Epoch 80, training loss: 0.03736456483602524
acc_val: 0.7100
Epoch 90, training loss: 0.03885002061724663
acc_val: 0.7125
Epoch 100, training loss: 0.035805922001600266
acc_val: 0.7100
Epoch 110, training loss: 0.030317259952425957
acc_val: 0.7150
Epoch 120, training loss: 0.03426198661327362
acc_val: 0.7075
Epoch 130, training loss: 0.03937900438904762
acc_val: 0.7050
Epoch 140, training loss: 0.03158750757575035
acc_val: 0.7025
Epoch 150, training loss: 0.02819123864173889
acc_val: 0.7025
Epoch 160, training loss: 0.027762535959482193
acc_val: 0.6950
Epoch 170, training loss: 0.026770232245326042
acc_val: 0.7050
Epoch 180, training loss: 0.02878861501812935
acc_val: 0.6900
Epoch 190, training loss: 0.03241918981075287
acc_val: 0.6950
=== picking the best model according to the performance on validation ===
Training benign model Finished!
Total time elapsed: 4.8550s
Benign model saved at ./modelpath/GCN_citeseer_benign.pth
Benign CA: 0.7100
Benign CA on clean test nodes: 0.6450
3327
200
Epoch 0, training loss: 1.790500283241272
acc_train_clean: 0.2146, acc_train_attach: 0.1304
Epoch 10, training loss: 0.26244547963142395
acc_train_clean: 0.9500, acc_train_attach: 0.9565
Epoch 20, training loss: 0.0915336012840271
acc_train_clean: 0.9792, acc_train_attach: 0.9565
Epoch 30, training loss: 0.06203740835189819
acc_train_clean: 0.9833, acc_train_attach: 1.0000
Epoch 40, training loss: 0.04943143576383591
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 50, training loss: 0.057540714740753174
acc_train_clean: 0.9833, acc_train_attach: 0.9565
Epoch 60, training loss: 0.06500133872032166
acc_train_clean: 0.9917, acc_train_attach: 0.9130
Epoch 70, training loss: 0.048253897577524185
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 80, training loss: 0.049989961087703705
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 90, training loss: 0.04556676372885704
acc_train_clean: 0.9896, acc_train_attach: 1.0000
Epoch 100, training loss: 0.045794107019901276
acc_train_clean: 0.9875, acc_train_attach: 1.0000
Epoch 110, training loss: 0.04852089285850525
acc_train_clean: 0.9854, acc_train_attach: 1.0000
Epoch 120, training loss: 0.044538382440805435
acc_train_clean: 0.9854, acc_train_attach: 1.0000
Epoch 130, training loss: 0.03526146337389946
acc_train_clean: 0.9875, acc_train_attach: 1.0000
Epoch 140, training loss: 0.05957728996872902
acc_train_clean: 0.9792, acc_train_attach: 1.0000
Epoch 150, training loss: 0.03485385701060295
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 160, training loss: 0.032307032495737076
acc_train_clean: 0.9917, acc_train_attach: 1.0000
Epoch 170, training loss: 0.03811737149953842
acc_train_clean: 0.9875, acc_train_attach: 1.0000
Epoch 180, training loss: 0.03762228786945343
acc_train_clean: 0.9917, acc_train_attach: 0.9565
Epoch 190, training loss: 0.035910967737436295
acc_train_clean: 0.9917, acc_train_attach: 1.0000
503
503
4454 4454
{1152, 2062, 1945, 3239, 1461, 57, 2371, 195, 2118, 1478, 1481, 2506, 1741, 1366, 3031, 1115, 3038, 3294, 350, 2537, 1778, 499, 1785}
=== training gcn model ===
Epoch 0, training loss: 1.7952362298965454
acc_val: 0.0900
Epoch 10, training loss: 0.30838656425476074
acc_val: 0.6800
Epoch 20, training loss: 0.10232729464769363
acc_val: 0.6775
Epoch 30, training loss: 0.06845837831497192
acc_val: 0.6975
Epoch 40, training loss: 0.04518524929881096
acc_val: 0.6950
Epoch 50, training loss: 0.0472295805811882
acc_val: 0.6950
Epoch 60, training loss: 0.05465896800160408
acc_val: 0.6925
Epoch 70, training loss: 0.05386113002896309
acc_val: 0.6975
Epoch 80, training loss: 0.0523119792342186
acc_val: 0.7025
Epoch 90, training loss: 0.0452633798122406
acc_val: 0.6975
Epoch 100, training loss: 0.04668483883142471
acc_val: 0.7000
Epoch 110, training loss: 0.03637545183300972
acc_val: 0.6950
Epoch 120, training loss: 0.043570421636104584
acc_val: 0.6825
Epoch 130, training loss: 0.044679906219244
acc_val: 0.6900
Epoch 140, training loss: 0.03911497816443443
acc_val: 0.6900
Epoch 150, training loss: 0.03547406569123268
acc_val: 0.6900
Epoch 160, training loss: 0.03854469209909439
acc_val: 0.6850
Epoch 170, training loss: 0.037945352494716644
acc_val: 0.6950
Epoch 180, training loss: 0.04174978658556938
acc_val: 0.6850
Epoch 190, training loss: 0.03342492878437042
acc_val: 0.6850
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7150
ASR: 1.0000
CA: 0.7050
