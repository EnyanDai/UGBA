nohup: ignoring input
/home/project-graph-backdoor/Backdoor/models/GAT.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
Namespace(clean_test_nodes_num=200, cuda=True, dataset='pubmed', debug=True, defense_mode='prune', device_id=5, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.6, homo_loss_weight=0.0, load_benign_model=True, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.15, seed=10, target_class=0, target_test_nodes_num=200, test_model='GAT', thrd=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)
Loading benign GCN model Finished!
Benign CA: 0.8110
Benign CA on clean test nodes: 0.7400
19717
200
Epoch 0, training loss: 1.098490834236145
acc_train_clean: 0.2083, acc_train_attach: 0.3155
Epoch 10, training loss: 0.6075447201728821
acc_train_clean: 0.3333, acc_train_attach: 1.0000
Epoch 20, training loss: 0.5429587364196777
acc_train_clean: 0.5583, acc_train_attach: 1.0000
Epoch 30, training loss: 0.4601385295391083
acc_train_clean: 0.8042, acc_train_attach: 1.0000
Epoch 40, training loss: 0.37842512130737305
acc_train_clean: 0.8542, acc_train_attach: 1.0000
Epoch 50, training loss: 0.31342199444770813
acc_train_clean: 0.8625, acc_train_attach: 1.0000
Epoch 60, training loss: 0.2580145001411438
acc_train_clean: 0.8750, acc_train_attach: 1.0000
Epoch 70, training loss: 0.24708066880702972
acc_train_clean: 0.8708, acc_train_attach: 1.0000
Epoch 80, training loss: 0.21780793368816376
acc_train_clean: 0.9125, acc_train_attach: 1.0000
Epoch 90, training loss: 0.1987154483795166
acc_train_clean: 0.9292, acc_train_attach: 0.9947
Epoch 100, training loss: 0.20866759121418
acc_train_clean: 0.9167, acc_train_attach: 0.9947
Epoch 110, training loss: 0.23188872635364532
acc_train_clean: 0.9208, acc_train_attach: 1.0000
Epoch 120, training loss: 0.1986335963010788
acc_train_clean: 0.9375, acc_train_attach: 0.9947
Epoch 130, training loss: 0.16939203441143036
acc_train_clean: 0.9458, acc_train_attach: 1.0000
Epoch 140, training loss: 0.1529444009065628
acc_train_clean: 0.9583, acc_train_attach: 1.0000
Epoch 150, training loss: 0.1574837863445282
acc_train_clean: 0.9583, acc_train_attach: 0.9947
Epoch 160, training loss: 0.1521543562412262
acc_train_clean: 0.9458, acc_train_attach: 1.0000
Epoch 170, training loss: 0.12763607501983643
acc_train_clean: 0.9708, acc_train_attach: 1.0000
Epoch 180, training loss: 0.13360822200775146
acc_train_clean: 0.9542, acc_train_attach: 1.0000
Epoch 190, training loss: 0.12905189394950867
acc_train_clean: 0.9667, acc_train_attach: 1.0000
427
427
81052 57604
{11777, 10246, 7175, 17928, 15370, 13326, 12816, 17914, 2067, 15892, 1530, 3606, 10776, 15385, 17438, 1566, 12833, 16419, 18983, 13352, 4650, 45, 15919, 4145, 8753, 8755, 6201, 9785, 12864, 64, 10307, 4676, 7754, 3147, 16460, 19533, 9294, 1101, 4176, 7245, 19539, 13395, 4179, 4692, 10839, 15448, 3680, 3681, 3170, 7274, 9324, 11373, 14454, 19584, 128, 12419, 10884, 4742, 2183, 11400, 6283, 1680, 18066, 6805, 6294, 5277, 11935, 15009, 10408, 16552, 19122, 8371, 14005, 14011, 19132, 8905, 9930, 13517, 7374, 12494, 18642, 5332, 7384, 8926, 6369, 226, 9445, 235, 4849, 753, 9460, 7414, 10492, 11007, 18176, 16130, 3845, 14086, 16645, 9993, 1296, 7959, 15640, 15132, 18722, 15145, 8494, 7473, 2354, 5427, 6964, 13621, 6455, 1855, 4933, 8522, 15178, 14156, 11084, 18250, 16717, 9043, 12627, 16728, 9563, 12637, 8547, 5987, 16741, 2917, 4970, 18799, 14193, 17778, 3955, 17266, 374, 14713, 6012, 2941, 16256, 12164, 6532, 3974, 18311, 8588, 19341, 19350, 17303, 3481, 7580, 17824, 14753, 8609, 17316, 6565, 7592, 6570, 17835, 18348, 2478, 6575, 9141, 14263, 14780, 12221, 15294, 8646, 17865, 6606, 3541, 2520, 9181, 5092, 6628, 18918, 11751, 15848, 15845, 4076, 4589, 2543, 5103, 10226, 2554, 6651, 3068}
=== training gcn model ===
Epoch 0, training loss: 1.1138871908187866
acc_val: 0.2275
Epoch 10, training loss: 0.8149476051330566
acc_val: 0.2800
Epoch 20, training loss: 0.680870771408081
acc_val: 0.4750
Epoch 30, training loss: 0.6774757504463196
acc_val: 0.4475
Epoch 40, training loss: 0.606158435344696
acc_val: 0.3975
Epoch 50, training loss: 0.5896025896072388
acc_val: 0.3850
Epoch 60, training loss: 0.5566221475601196
acc_val: 0.3850
Epoch 70, training loss: 0.6059284210205078
acc_val: 0.4200
Epoch 80, training loss: 0.5540608763694763
acc_val: 0.3950
Epoch 90, training loss: 0.5233485698699951
acc_val: 0.4575
Epoch 100, training loss: 0.5624665021896362
acc_val: 0.3825
Epoch 110, training loss: 0.5508314967155457
acc_val: 0.4300
Epoch 120, training loss: 0.5442660450935364
acc_val: 0.3825
Epoch 130, training loss: 0.5908815264701843
acc_val: 0.4725
Epoch 140, training loss: 0.5614558458328247
acc_val: 0.4075
Epoch 150, training loss: 0.5668389797210693
acc_val: 0.4050
Epoch 160, training loss: 0.5458880662918091
acc_val: 0.4100
Epoch 170, training loss: 0.5537406802177429
acc_val: 0.4200
Epoch 180, training loss: 0.5931018590927124
acc_val: 0.3650
Epoch 190, training loss: 0.6221871376037598
acc_val: 0.4200
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8877
accuracy on clean test nodes: 0.4500
ASR: 0.5050
CA: 0.4400
