nohup: ignoring input
Namespace(clean_test_nodes_num=200, cuda=True, dataset='cora', debug=True, defense_mode='prune', device_id=5, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.6, homo_loss_weight=1.0, load_benign_model=True, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.1, seed=10, target_class=0, target_test_nodes_num=200, test_model='GAT', thrd=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)
Loading benign GCN model Finished!
Benign CA: 0.8360
Benign CA on clean test nodes: 0.7800
2708
200
tensor(1.9395, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(20.4000, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 0, training loss: 1.9394763708114624
acc_train_clean: 0.1768, acc_train_attach: 0.0000
tensor(1.7910, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(8.4708, device='cuda:5', grad_fn=<MulBackward0>)
tensor(1.6239, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(12.5005, device='cuda:5', grad_fn=<MulBackward0>)
tensor(1.4373, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(12.0582, device='cuda:5', grad_fn=<MulBackward0>)
tensor(1.2905, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(10.7606, device='cuda:5', grad_fn=<MulBackward0>)
tensor(1.1322, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(9.8844, device='cuda:5', grad_fn=<MulBackward0>)
tensor(1.0009, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(8.1433, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.8901, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(6.6891, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.7913, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(5.6920, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.7016, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(4.4478, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.5769, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(3.7621, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 10, training loss: 0.5769366025924683
acc_train_clean: 0.9089, acc_train_attach: 0.9412
tensor(0.5221, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(3.2531, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.4807, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(2.9389, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.4095, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(2.6986, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.3534, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(2.1517, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.3269, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.9518, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.2999, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.9121, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.2844, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.5390, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.2460, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.6180, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.2453, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.4569, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.2168, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.3461, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 20, training loss: 0.2168077826499939
acc_train_clean: 0.9411, acc_train_attach: 1.0000
tensor(0.2165, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.4080, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1971, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.0009, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1871, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.7671, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1906, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.9759, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1520, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.9168, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1508, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.1134, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1627, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.7089, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1295, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.7336, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1398, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.9197, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1352, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.5698, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 30, training loss: 0.1352093517780304
acc_train_clean: 0.9589, acc_train_attach: 0.9412
tensor(0.1215, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(1.0396, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1300, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.6746, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1186, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.5072, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1010, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4422, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1154, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.8181, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1042, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.7034, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1175, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3835, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1005, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4051, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0963, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4506, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1041, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.5282, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 40, training loss: 0.10413007438182831
acc_train_clean: 0.9768, acc_train_attach: 0.8824
tensor(0.1011, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.6398, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0824, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3598, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.1107, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4254, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0950, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.6078, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0938, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4539, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0873, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3195, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0905, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3056, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0781, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3835, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0747, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.5644, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0785, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4783, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 50, training loss: 0.07850217074155807
acc_train_clean: 0.9857, acc_train_attach: 0.9412
tensor(0.0776, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2392, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0829, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3922, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0914, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2678, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0777, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2392, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0813, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3345, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0859, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2573, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0738, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1674, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0840, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1555, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0712, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.4254, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0959, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1743, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 60, training loss: 0.09586644172668457
acc_train_clean: 0.9750, acc_train_attach: 0.9412
tensor(0.0804, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0821, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1419, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0748, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0999, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0724, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2845, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0858, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0858, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0859, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1199, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0723, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0845, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0501, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0778, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1199, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 70, training loss: 0.0778297409415245
acc_train_clean: 0.9786, acc_train_attach: 1.0000
tensor(0.0752, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0752, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1880, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0846, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0704, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0759, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2076, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0671, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0718, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0684, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1153, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0854, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0826, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1031, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0723, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 80, training loss: 0.07234152406454086
acc_train_clean: 0.9839, acc_train_attach: 0.9412
tensor(0.0680, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1764, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0744, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2241, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0779, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0790, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0742, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0718, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0682, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0716, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0673, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0675, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0501, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0645, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3128, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0644, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 90, training loss: 0.06441794335842133
acc_train_clean: 0.9893, acc_train_attach: 1.0000
tensor(0.0719, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0875, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2544, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0669, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0697, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3780, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0743, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0813, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2059, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0731, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1572, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0706, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0673, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0722, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2415, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 100, training loss: 0.07217559963464737
acc_train_clean: 0.9857, acc_train_attach: 1.0000
tensor(0.0607, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1581, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0700, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0680, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0743, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3066, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0655, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0966, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0639, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0620, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0642, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1499, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0716, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2761, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0612, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1606, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 110, training loss: 0.061196886003017426
acc_train_clean: 0.9839, acc_train_attach: 1.0000
tensor(0.0627, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0617, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0559, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0606, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0620, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1555, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0574, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1417, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0591, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0215, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0700, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2688, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0694, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0501, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0712, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2522, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0606, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 120, training loss: 0.06062088906764984
acc_train_clean: 0.9893, acc_train_attach: 0.9412
tensor(0.0585, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0626, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0699, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1633, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0550, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0543, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0648, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0627, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0607, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0682, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1546, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0587, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2217, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0596, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0709, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 130, training loss: 0.0595700666308403
acc_train_clean: 0.9875, acc_train_attach: 1.0000
tensor(0.0564, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0587, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0521, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0629, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0501, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0602, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0565, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0671, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0614, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0978, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0604, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0553, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0598, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 140, training loss: 0.059813667088747025
acc_train_clean: 0.9893, acc_train_attach: 1.0000
tensor(0.0577, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0623, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2425, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0570, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0561, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0608, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0568, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0510, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0634, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0627, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0629, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 150, training loss: 0.06288408488035202
acc_train_clean: 0.9857, acc_train_attach: 1.0000
tensor(0.0557, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0581, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1555, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0573, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1279, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0665, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3056, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0667, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0551, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0575, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0547, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0241, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0584, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0591, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2000, device='cuda:5', grad_fn=<MulBackward0>)
Epoch 160, training loss: 0.05911751464009285
acc_train_clean: 0.9893, acc_train_attach: 1.0000
tensor(0.0538, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1912, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0563, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0515, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0670, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0491, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0597, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0591, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0639, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0881, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0466, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0345, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0591, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0532, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 170, training loss: 0.0531567819416523
acc_train_clean: 0.9911, acc_train_attach: 0.8824
tensor(0.0535, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0820, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0517, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1075, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0578, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0485, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0470, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0552, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1505, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0550, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0550, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1858, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0632, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0529, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0601, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 180, training loss: 0.0601336695253849
acc_train_clean: 0.9893, acc_train_attach: 1.0000
tensor(0.0498, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0495, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0098, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0515, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.1046, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0503, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0954, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0576, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0505, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0521, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0529, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0517, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0168, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0504, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
Epoch 190, training loss: 0.05044815316796303
acc_train_clean: 0.9875, acc_train_attach: 0.9412
tensor(0.0486, device='cuda:5', grad_fn=<NllLossBackward0>) /home/project-graph-backdoor/Backdoor/models/GAT.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
tensor(0.0048, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0505, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0118, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0512, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0611, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2008, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0556, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0564, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0579, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.2495, device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0512, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:5', grad_fn=<MulBackward0>)
tensor(0.0460, device='cuda:5', grad_fn=<NllLossBackward0>) tensor(0.3678, device='cuda:5', grad_fn=<MulBackward0>)
577
577
4306 2880
{2242, 1506, 2436, 1157, 1734, 2276, 1029, 2089, 1894, 2502, 430, 1743, 1969, 213, 1591, 183, 2204}
=== training gcn model ===
Epoch 0, training loss: 2.0440878868103027
acc_val: 0.4975
Epoch 10, training loss: 0.7885571122169495
acc_val: 0.7700
Epoch 20, training loss: 0.6746844053268433
acc_val: 0.7700
Epoch 30, training loss: 0.6699784994125366
acc_val: 0.7475
Epoch 40, training loss: 0.6360043883323669
acc_val: 0.7425
Epoch 50, training loss: 0.6625087857246399
acc_val: 0.7400
Epoch 60, training loss: 0.6622152328491211
acc_val: 0.7250
Epoch 70, training loss: 0.6995226740837097
acc_val: 0.7175
Epoch 80, training loss: 0.6901212930679321
acc_val: 0.7275
Epoch 90, training loss: 0.565118134021759
acc_val: 0.7200
Epoch 100, training loss: 0.6062511205673218
acc_val: 0.7400
Epoch 110, training loss: 0.6375568509101868
acc_val: 0.7075
Epoch 120, training loss: 0.6016472578048706
acc_val: 0.7300
Epoch 130, training loss: 0.6328696012496948
acc_val: 0.7200
Epoch 140, training loss: 0.5843803882598877
acc_val: 0.7250
Epoch 150, training loss: 0.6830222606658936
acc_val: 0.7275
Epoch 160, training loss: 0.6273492574691772
acc_val: 0.7350
Epoch 170, training loss: 0.6044825911521912
acc_val: 0.7175
Epoch 180, training loss: 0.672744870185852
acc_val: 0.7400
Epoch 190, training loss: 0.6192070841789246
acc_val: 0.7375
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.9412
accuracy on clean test nodes: 0.7900
ASR: 0.5300
CA: 0.7350
