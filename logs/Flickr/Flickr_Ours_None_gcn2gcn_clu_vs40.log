nohup: ignoring input
Namespace(cuda=True, dataset='Flickr', debug=True, defense_mode='none', device_id=0, dis_weight=0.0, dropout=0.5, epochs=200, evaluate_mode='overall', hidden=64, homo_boost_thrd=0.5, homo_loss_weight=1.0, inner=1, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.15, seed=11, selection_method='cluster_degree', target_class=0, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.02, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)
Length of training set: 17850
Traceback (most recent call last):
  File "run_adaptive.py", line 145, in <module>
    benign_model.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)
  File "/home/mfl5681/project-backdoor/Backdoor/models/GCN.py", line 92, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters, verbose)
  File "/home/mfl5681/project-backdoor/Backdoor/models/GCN.py", line 123, in _train_with_val
    output = self.forward(self.features, self.edge_index, self.edge_weight)
  File "/home/mfl5681/project-backdoor/Backdoor/models/GCN.py", line 49, in forward
    x = F.relu(conv(x, edge_index,edge_weight))
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py", line 197, in forward
    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py", line 374, in propagate
    out = self.message(**msg_kwargs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py", line 206, in message
    return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j
RuntimeError: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 47.54 GiB total capacity; 398.12 MiB already allocated; 94.31 MiB free; 424.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
