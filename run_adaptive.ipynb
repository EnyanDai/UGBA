{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='prune', device_id=0, dis_weight=1, dropout=0.5, epochs=200, evaluate_mode='1by1', hidden=32, homo_boost_thrd=0.8, homo_loss_weight=100, inner=1, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=400, use_vs_number=True, vs_number=40, vs_ratio=0, weight_decay=0.0005)\n",
      "#Attach Nodes:40\n",
      "Length of training set: 33868\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 3.723762273788452\n",
      "acc_val: 0.1627\n",
      "Epoch 10, training loss: 3.1244382858276367\n",
      "acc_val: 0.2077\n",
      "Epoch 20, training loss: 2.903806686401367\n",
      "acc_val: 0.2803\n",
      "Epoch 30, training loss: 2.5592551231384277\n",
      "acc_val: 0.3181\n",
      "Epoch 40, training loss: 2.3185300827026367\n",
      "acc_val: 0.4280\n",
      "Epoch 50, training loss: 2.1377949714660645\n",
      "acc_val: 0.4724\n",
      "Epoch 60, training loss: 2.045152187347412\n",
      "acc_val: 0.4978\n",
      "Epoch 70, training loss: 1.9729667901992798\n",
      "acc_val: 0.5165\n",
      "Epoch 80, training loss: 1.9153220653533936\n",
      "acc_val: 0.5401\n",
      "Epoch 90, training loss: 1.85359787940979\n",
      "acc_val: 0.5531\n",
      "Epoch 100, training loss: 1.813212275505066\n",
      "acc_val: 0.5691\n",
      "Epoch 110, training loss: 1.7736619710922241\n",
      "acc_val: 0.5774\n",
      "Epoch 120, training loss: 1.7632701396942139\n",
      "acc_val: 0.5767\n",
      "Epoch 130, training loss: 1.7372093200683594\n",
      "acc_val: 0.5794\n",
      "Epoch 140, training loss: 1.7211759090423584\n",
      "acc_val: 0.5837\n",
      "Epoch 150, training loss: 1.7109240293502808\n",
      "acc_val: 0.5873\n",
      "Epoch 160, training loss: 1.704589605331421\n",
      "acc_val: 0.5928\n",
      "Epoch 170, training loss: 1.6928389072418213\n",
      "acc_val: 0.5932\n",
      "Epoch 180, training loss: 1.685895562171936\n",
      "acc_val: 0.5936\n",
      "Epoch 190, training loss: 1.6810754537582397\n",
      "acc_val: 0.5979\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 3.8706s\n",
      "Encoder CA on clean test nodes: 0.6045\n",
      "[24 34 28 ... 24  4  8]\n",
      "idx_attach: tensor([  4026, 131515, 131368,  ..., 106699,  25208,  67166], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 169>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# In[10]:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m# train trigger generator \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m model \u001b[39m=\u001b[39m Backdoor(args,device)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data\u001b[39m.\u001b[39;49mx, train_edge_index, \u001b[39mNone\u001b[39;49;00m, data\u001b[39m.\u001b[39;49my, idx_train,idx_attach, unlabeled_idx)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m poison_x, poison_edge_index, poison_edge_weights, poison_labels \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_poisoned()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bxzz89-02-mfl5681/home/mfl5681/project-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mif\u001b[39;00m(args\u001b[39m.\u001b[39mdefense_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprune\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/project-backdoor/Backdoor/models/backdoor.py:234\u001b[0m, in \u001b[0;36mBackdoor.fit\u001b[0;34m(self, features, edge_index, edge_weight, labels, idx_train, idx_attach, idx_unlabeled)\u001b[0m\n\u001b[1;32m    231\u001b[0m optimizer_trigger\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    233\u001b[0m rs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mseed)\n\u001b[0;32m--> 234\u001b[0m idx_outter \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([idx_attach,idx_unlabeled[rs\u001b[39m.\u001b[39;49mchoice(\u001b[39mlen\u001b[39;49m(idx_unlabeled),size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,replace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)]])\n\u001b[1;32m    236\u001b[0m trojan_feat, trojan_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrojan(features[idx_outter],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mthrd) \u001b[39m# may revise the process of generate\u001b[39;00m\n\u001b[1;32m    238\u001b[0m trojan_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mones([\u001b[39mlen\u001b[39m(idx_outter),\u001b[39m1\u001b[39m],dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat,device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),trojan_weights],dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32mmtrand.pyx:909\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "\n",
    "import imp\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.datasets import Planetoid,Reddit2,Flickr,PPI\n",
    "\n",
    "\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='ogbn-arxiv', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=400, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=True,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=40,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"prune\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=100,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.8,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='1by1',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=0,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    from ogb.nodeproppred import PygNodePropPredDataset\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "#%% \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import Backdoor\n",
    "from models.construct import model_construct\n",
    "import heuristic_selection as hs\n",
    "\n",
    "# from kmeans_pytorch import kmeans, kmeans_predict\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "if(args.use_vs_number):\n",
    "    size = args.vs_number\n",
    "else:\n",
    "    size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(\"#Attach Nodes:{}\".format(size))\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = hs.obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    idx_attach = hs.cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster_degree'):\n",
    "    if(args.dataset == 'Cora' or args.dataset == 'Pubmed'):\n",
    "        idx_attach = hs.cluster_degree_selection_seperate_fixed(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    else:\n",
    "        idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    # idx_attach = hs.cluster_degree_selection_seperate(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    # idx_attach = hs.cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "print(\"idx_attach: {}\".format(idx_attach))\n",
    "unlabeled_idx = torch.tensor(list(set(unlabeled_idx.cpu().numpy()) - set(idx_attach.cpu().numpy()))).to(device)\n",
    "# In[10]:\n",
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "\n",
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device,large_graph=False)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device,large_graph=False)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "    .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "\n",
    "\n",
    "models = ['GCN','GAT', 'GraphSage']\n",
    "total_overall_asr = 0\n",
    "total_overall_ca = 0\n",
    "for test_model in models:\n",
    "    args.test_model = test_model\n",
    "    rs = np.random.RandomState(args.seed)\n",
    "    # seeds = rs.randint(1000,size=5)\n",
    "    seeds = [args.seed]\n",
    "    overall_asr = 0\n",
    "    overall_ca = 0\n",
    "    for seed in seeds:\n",
    "        args.seed = seed\n",
    "        # np.random.seed(seed)\n",
    "        # torch.manual_seed(seed)\n",
    "        # torch.cuda.manual_seed(seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        print(args)\n",
    "        #%%\n",
    "        test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "        # TODO: add multiple time seeds\n",
    "        test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "        output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "        print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "        # torch.cuda.empty_cache()\n",
    "        #%%\n",
    "        induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "        induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "        clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        # test_model = test_model.cpu()\n",
    "\n",
    "        print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "\n",
    "        if(args.evaluate_mode == '1by1'):\n",
    "            from torch_geometric.utils  import k_hop_subgraph\n",
    "            overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "            asr = 0\n",
    "            flip_asr = 0\n",
    "            flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "            for i, idx in enumerate(idx_atk):\n",
    "                idx=int(idx)\n",
    "                sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "                ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "                relabeled_node_idx = sub_mapping\n",
    "                sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "                with torch.no_grad():\n",
    "                    # inject trigger on attack test nodes (idx_atk)'''\n",
    "                    induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "                    induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "                    # # do pruning in test datas'''\n",
    "                    if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                        induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device,False)\n",
    "                    # attack evaluation\n",
    "\n",
    "                    output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "                    train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "                    # print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "                    # print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "                    asr += train_attach_rate\n",
    "                    if(data.y[idx] != args.target_class):\n",
    "                        flip_asr += train_attach_rate\n",
    "            asr = asr/(idx_atk.shape[0])\n",
    "            flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "            print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "            print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "        elif(args.evaluate_mode == 'overall'):\n",
    "            # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "            induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "            # do pruning in test datas'''\n",
    "            if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "                induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "            # attack evaluation\n",
    "\n",
    "            # test_model = test_model.to(device)\n",
    "            output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "            train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "            print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "            asr = train_attach_rate\n",
    "            flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "            flip_asr = (output.argmax(dim=1)[flip_idx_atk]==args.target_class).float().mean()\n",
    "            print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "            ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "            print(\"CA: {:.4f}\".format(ca))\n",
    "        \n",
    "        overall_asr += asr\n",
    "        overall_ca += clean_acc\n",
    "    overall_asr = overall_asr/len(seeds)\n",
    "    overall_ca = overall_ca/len(seeds)\n",
    "    print(\"Overall ASR: {:.4f} ({} model, Seed: {})\".format(overall_asr, args.test_model, args.seed))\n",
    "    print(\"Overall Clean Accuracy: {:.4f}\".format(overall_ca))\n",
    "\n",
    "    total_overall_asr += overall_asr\n",
    "    total_overall_ca += overall_ca\n",
    "    test_model.to(torch.device('cpu'))\n",
    "total_overall_asr = total_overall_asr/len(models)\n",
    "total_overall_ca = total_overall_ca/len(models)\n",
    "print(\"Total Overall ASR: {:.4f} \".format(total_overall_asr))\n",
    "print(\"Total Clean Accuracy: {:.4f}\".format(total_overall_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "# if(args.attack_method == 'none'):\n",
    "#     bkd_tn_nodes = idx_train\n",
    "print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "    .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "#%%\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "# test_model = test_model.cpu()\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "# poison_x, poison_edge_index, poison_edge_weights, poison_labels = poison_x.to(device2), poison_edge_index.to(device2), poison_edge_weights.to(device2), poison_labels.to(device2)\n",
    "# model.trojan = model.trojan.cpu()\n",
    "if(args.evaluate_mode == '1by1'):\n",
    "    from torch_geometric.utils  import k_hop_subgraph\n",
    "    overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "    asr = 0\n",
    "    flip_asr = 0\n",
    "    flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "    for i, idx in enumerate(idx_atk):\n",
    "        idx=int(idx)\n",
    "        sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "        ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "        relabeled_node_idx = sub_mapping\n",
    "        sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "        # inject trigger on attack test nodes (idx_atk)'''\n",
    "        if(args.attack_method == 'Basic'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "        elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,data.y[sub_induct_nodeset], full_data=True)\n",
    "        elif(args.attack_method == 'None'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights\n",
    "\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        # test_model = test_model.to(device)\n",
    "        output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "        print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        asr += train_attach_rate\n",
    "        if(data.y[idx] != args.target_class):\n",
    "            flip_asr += train_attach_rate\n",
    "        # ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        # print(\"CA: {:.4f}\".format(ca))\n",
    "    asr = asr/(idx_atk.shape[0])\n",
    "    flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "    print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "    print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "elif(args.evaluate_mode == 'overall'):\n",
    "    # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "    if(args.attack_method == 'Basic'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "    elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(idx_atk,poison_x,induct_edge_index,induct_edge_weights,data.y)\n",
    "    elif(args.attack_method == 'None'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = poison_x,induct_edge_index,induct_edge_weights\n",
    "\n",
    "    induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "    # do pruning in test datas'''\n",
    "    if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "        induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "    # attack evaluation\n",
    "\n",
    "    # test_model = test_model.to(device)\n",
    "    output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "    print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "    ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign model saved at ./modelpath/GCN_Reddit2_benign.pth\n",
      "Benign CA: 0.7006\n"
     ]
    }
   ],
   "source": [
    "torch.save(benign_model, benign_modelpath)\n",
    "print(\"Benign model saved at {}\".format(benign_modelpath))\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, None, data.y,idx_clean_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "benign_model = benign_model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# splitted_idx = data.get_idx_split()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m idx_train, idx_val, idx_test \u001b[39m=\u001b[39m split_idx[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m graph, labels \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./dataset/')\n",
    "\n",
    "split_idx = dataset.get_idx_split() \n",
    "# splitted_idx = data.get_idx_split()\n",
    "idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n",
    "# train_loader = DataLoader(dataset[split_idx['train']])\n",
    "# valid_loader = DataLoader(dataset[split_idx['valid']])\n",
    "# test_loader = DataLoader(dataset[split_idx['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "# if args.dataset in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "dataset = Planetoid(root='./data/', split=\"random\", num_train_per_class=80, num_val=400, num_test=1000, \\\n",
    "                    name=args.dataset,transform=None)\n",
    "# dataset = Reddit(root='./data/', transform=transform, pre_transform=None)\n",
    "# dataset = classFlickr(root='./data/', transform=transform, pre_transform=None)\n",
    "\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "# get the overall edge index of the graph\n",
    "data.edge_index = to_undirected(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  mask the test nodes\n",
    "from utils import subgraph\n",
    "# get the edge index used for training (except from test nodes) and \n",
    "train_edge_index,train_edge_weights, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "idx_train =data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_test = data.test_mask.nonzero().flatten()\n",
    "# val_mask = node_idx[data.val_mask]\n",
    "# labels = data.y[torch.bitwise_not(data.test_mask)]\n",
    "# features = data.x[torch.bitwise_not(data.test_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028799998253884154\n"
     ]
    }
   ],
   "source": [
    "e = data.edge_index.shape[1]\n",
    "t = data.x.shape[0]\n",
    "p = 2*e/(t*(t-1))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "from models.SAGE import GraphSage\n",
    "def model_construct(args,model_name,data):\n",
    "    if (model_name == 'GCN'):\n",
    "        model = GCN(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GAT'):\n",
    "        model = GAT(nfeat=data.x.shape[1], \n",
    "                    nhid=args.hidden, \n",
    "                    nclass=int(data.y.max()+1), \n",
    "                    heads=8,\n",
    "                    dropout=args.dropout, \n",
    "                    lr=args.lr, \n",
    "                    weight_decay=args.weight_decay, \n",
    "                    device=device)\n",
    "    elif(model_name == 'GraphSage'):\n",
    "        model = GraphSage(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GCN_Encoder'):\n",
    "        model = GCN_Encoder(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train benign model\n",
    "'''\n",
    "import os\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath) and args.load_benign_model):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    benign_model = model_construct(args,args.model,data).to(device) \n",
    "    t_total = time.time()\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    torch.save(benign_model, benign_modelpath)\n",
    "    print(\"Benign model saved at {}\".format(benign_modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign CA: 0.8410\n",
      "Benign CA on clean test nodes: 0.8250\n"
     ]
    }
   ],
   "source": [
    "benign_output = benign_model(data.x, data.edge_index, edge_weights)\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "atk_test_nodes, clean_test_nodes,poi_train_nodes = select_target_nodes(args,args.seed,benign_model,data.x, data.edge_index, edge_weights,data.y,idx_val,idx_test)\n",
    "clean_test_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "print(\"Benign CA on clean test nodes: {:.4f}\".format(clean_test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n",
      "Encoder CA: 0.8380\n",
      "Encoder CA on clean test nodes: 0.8000\n",
      "[4 4 3 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import obtain_attach_nodes,Backdoor,obtain_attach_nodes_by_influential,obtain_attach_nodes_by_cluster\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "# poison nodes' size\n",
    "size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,train_edge_index,train_edge_weights,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    # construct GCN encoder\n",
    "    encoder_modelpath = './modelpath/{}_{}_benign.pth'.format('GCN_Encoder', args.dataset)\n",
    "    if(os.path.exists(encoder_modelpath) and args.load_benign_model):\n",
    "        # load existing benign model\n",
    "        gcn_encoder = torch.load(encoder_modelpath)\n",
    "        gcn_encoder = gcn_encoder.to(device)\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "    else:\n",
    "        gcn_encoder = model_construct(args,'GCN_Encoder',data).to(device) \n",
    "        t_total = time.time()\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "        gcn_encoder.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "        print(\"Training encoder Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        # Save trained model\n",
    "        torch.save(gcn_encoder, encoder_modelpath)\n",
    "        print(\"Encoder saved at {}\".format(encoder_modelpath))\n",
    "    # test gcn encoder \n",
    "    encoder_benign_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "    print(\"Encoder CA: {:.4f}\".format(encoder_benign_ca))\n",
    "    encoder_clean_test_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "    print(\"Encoder CA on clean test nodes: {:.4f}\".format(encoder_clean_test_ca))\n",
    "    # from sklearn import cluster\n",
    "    seen_node_idx = torch.concat([idx_train,unlabeled_idx])\n",
    "    nclass = np.unique(data.y.cpu().numpy()).shape[0]\n",
    "    # kmeans = cluster.KMeans(n_clusters=nclass,random_state=1)\n",
    "    # kmeans.fit(data.x[seen_node_idx].cpu().numpy())\n",
    "    # # unlabeled_idx.cpu().tolist()\n",
    "\n",
    "    # train_adj = to_dense_adj(train_edge_index,edge_attr=train_edge_weights)[0].cpu()\n",
    "    # train_adj = train_adj + train_adj @ train_adj\n",
    "    # train_adj = torch.where(train_adj>0, torch.tensor(1.0),\n",
    "    #                                             torch.tensor(0.0))\n",
    "    # train_x = train_adj @ data.x.cpu()\n",
    "    # new_train_edge_index, new_train_edge_weights= dense_to_sparse(train_adj)\n",
    "    # kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    # kmeans.fit(train_x.detach().cpu().numpy())\n",
    "    # idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),train_x,data.y,device,size)\n",
    "    # idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    \n",
    "    encoder_x = gcn_encoder.get_h(data.x, train_edge_index,train_edge_weights).clone().detach()\n",
    "    kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    kmeans.fit(encoder_x.detach().cpu().numpy())\n",
    "    idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),encoder_x,data.y,device,size)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor(1.9393, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(20.4000, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 0, training loss: 1.9392701387405396\n",
      "acc_train_clean: 0.1768, acc_train_attach: 0.0588\n",
      "tensor(1.7866, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.8495, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.6262, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(13.2455, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.4436, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(11.8224, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.3043, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(10.5700, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.1444, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.4848, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.0180, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(8.4280, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.9117, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(6.9387, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.8150, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(5.7199, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.7289, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(4.5158, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.6040, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.5913, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 10, training loss: 0.6040130853652954\n",
      "acc_train_clean: 0.9000, acc_train_attach: 0.7059\n",
      "tensor(0.5469, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.7172, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.5104, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.5027, device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "print(args.epochs)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "poison_x = model.poison_x.data\n",
    "poison_edge_index = model.poison_edge_index.data\n",
    "poison_edge_weights = model.poison_edge_weights.data\n",
    "poison_labels = model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "577\n",
      "4306 2880\n",
      "{1569, 2081, 1699, 1251, 133, 2185, 843, 1775, 303, 305, 1780, 1814, 1782, 2143, 1243, 2046, 1791}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(torch.cat([idx_train,idx_attach])))\n",
    "print(len(bkd_tn_nodes))\n",
    "print(len(model.poison_edge_index.data[0]),len(poison_edge_index[0]))\n",
    "# print(idx_attach & bkd_tn_nodes)\n",
    "print(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9486966133117676\n",
      "acc_val: 0.3725\n",
      "Epoch 10, training loss: 0.5837284326553345\n",
      "acc_val: 0.7350\n",
      "Epoch 20, training loss: 0.22602947056293488\n",
      "acc_val: 0.7625\n",
      "Epoch 30, training loss: 0.126153364777565\n",
      "acc_val: 0.7625\n",
      "Epoch 40, training loss: 0.09417053312063217\n",
      "acc_val: 0.7625\n",
      "Epoch 50, training loss: 0.08149732649326324\n",
      "acc_val: 0.7525\n",
      "Epoch 60, training loss: 0.08437832444906235\n",
      "acc_val: 0.7425\n",
      "Epoch 70, training loss: 0.07754302769899368\n",
      "acc_val: 0.7500\n",
      "Epoch 80, training loss: 0.08388686925172806\n",
      "acc_val: 0.7575\n",
      "Epoch 90, training loss: 0.0656595528125763\n",
      "acc_val: 0.7525\n",
      "Epoch 100, training loss: 0.0655829906463623\n",
      "acc_val: 0.7475\n",
      "Epoch 110, training loss: 0.06835032254457474\n",
      "acc_val: 0.7600\n",
      "Epoch 120, training loss: 0.060565438121557236\n",
      "acc_val: 0.7525\n",
      "Epoch 130, training loss: 0.07300901412963867\n",
      "acc_val: 0.7550\n",
      "Epoch 140, training loss: 0.05255601555109024\n",
      "acc_val: 0.7500\n",
      "Epoch 150, training loss: 0.05699390545487404\n",
      "acc_val: 0.7500\n",
      "Epoch 160, training loss: 0.0540340282022953\n",
      "acc_val: 0.7450\n",
      "Epoch 170, training loss: 0.060852691531181335\n",
      "acc_val: 0.7475\n",
      "Epoch 180, training loss: 0.05091455578804016\n",
      "acc_val: 0.7500\n",
      "Epoch 190, training loss: 0.064822718501091\n",
      "acc_val: 0.7450\n",
      "=== picking the best model according to the performance on validation ===\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "test_model = model_construct(args,args.test_model,data).to(device) \n",
    "if(args.test_model == 'GraphSage' or args.test_model == 'GAT'):\n",
    "    poison_adj = to_dense_adj(poison_edge_index, edge_attr=poison_edge_weights)\n",
    "    poison_edge_index, poison_edge_weights = dense_to_sparse(poison_adj)\n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=200,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target class rate on Vs: 0.8824\n",
      "accuracy on clean test nodes: 0.8050\n",
      "ASR: 0.7700\n",
      "CA: 0.7550\n"
     ]
    }
   ],
   "source": [
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "# idx_test = data.test_mask.nonzero().flatten()[:200]\n",
    "# idx_test = list(set(data.test_mask.nonzero().flatten().tolist()) - set(atk_test_nodes))\n",
    "# idx_atk = data.test_mask.nonzero().flatten()[200:].tolist()\n",
    "# yt_nids = [nid for nid in idx_atk if data.y.tolist()==args.target_class] \n",
    "# yx_nids = torch.LongTensor(list(set(idx_atk) - set(yt_nids))).to(device)\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[atk_test_nodes] = args.target_class\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "'''clean accuracy of clean test nodes before injecting triggers to the attack test nodes'''\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "'''inject trigger on attack test nodes (idx_atk)'''\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(atk_test_nodes,poison_x,induct_edge_index,induct_edge_weights)\n",
    "'''do pruning in test datas'''\n",
    "if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "    induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "'''attack evaluation'''\n",
    "asr = test_model.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,atk_test_nodes)\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "print(\"ASR: {:.4f}\".format(asr))\n",
    "print(\"CA: {:.4f}\".format(ca))\n",
    "# output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "# train_attach_rate = (output.argmax(dim=1)[atk_test_nodes]==args.target_class).float().mean()\n",
    "# print(\"ASR: {:.4f}\".format(train_attach_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "sp_induct_x = help_funcs.normalize(sp.csr_matrix(induct_x.cpu().detach().numpy()))\n",
    "sp_induct_adj = help_funcs.normalize_adj(sp.csr_matrix(to_dense_adj(induct_edge_index)[0].cpu().detach().numpy()))\n",
    "induct_x = torch.FloatTensor(np.array(sp_induct_x.todense())).to(device)\n",
    "induct_adj = torch.FloatTensor(np.array(sp_induct_adj.todense())).to(device)\n",
    "induct_edge_index,induct_edge_weights = dense_to_sparse(induct_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[yx_nids]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "clean_acc = gcn.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_test)\n",
    "asr = gcn.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,idx_atk)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"ASR1: {:.4f}\".format(asr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_gcn = GCN(nfeat=data.x.shape[1],\\\n",
    "            nhid=args.hidden,\\\n",
    "            nclass= int(data.y.max()+1),\\\n",
    "            dropout=args.dropout,\\\n",
    "            lr=args.lr,\\\n",
    "            weight_decay=args.weight_decay,\\\n",
    "            device=device).to(device)\n",
    "#%%\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[idx_atk] = args.target_class\n",
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "benign_gcn.fit(data.x, data.edge_index, edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "benign_output = benign_gcn(data.x, data.edge_index, edge_weights)\n",
    "benign4poison_output = benign_gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "benign_ca = (benign_output.argmax(dim=1)[idx_test]==data.y[idx_test]).float().mean()\n",
    "benign4poison_ca = (benign4poison_output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"BenignCA: {:.4f}\".format(benign_ca))\n",
    "print(\"Benign for poisoning CA: {:.4f}\".format(benign4poison_ca))\n",
    "print((benign_output.argmax(dim=1)[yx_nids]==args.target_class).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_labels = data.y.clone()\n",
    "idx_atk = obtain_attach_nodes(data.test_mask.nonzero().flatten(), 200)\n",
    "can_test_nodes = torch.LongTensor(list(set(data.test_mask.nonzero().flatten()) - set(idx_atk))).to(device)\n",
    "idx_test = obtain_attach_nodes(can_test_nodes,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,data.x,data.edge_index,edge_weights)\n",
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "test_asr= (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(test_asr))\n",
    "test_ca = (output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"CA: {:.4f}\".format(test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 29.81 GiB (GPU 1; 79.20 GiB total capacity; 779.04 MiB already allocated; 17.36 GiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_dense_adj\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m poison_adj_dense \u001b[39m=\u001b[39m to_dense_adj(poison_edge_index)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39medge_sim_analysis\u001b[39m(edge_index, features):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     sims \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/torch_geometric/utils/to_dense_adj.py:48\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes)\u001b[0m\n\u001b[1;32m     46\u001b[0m size \u001b[39m=\u001b[39m [batch_size, max_num_nodes, max_num_nodes]\n\u001b[1;32m     47\u001b[0m size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(edge_attr\u001b[39m.\u001b[39msize())[\u001b[39m1\u001b[39m:]\n\u001b[0;32m---> 48\u001b[0m adj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(size, dtype\u001b[39m=\u001b[39;49medge_attr\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49medge_index\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     50\u001b[0m flattened_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m max_num_nodes \u001b[39m*\u001b[39m max_num_nodes\n\u001b[1;32m     51\u001b[0m adj \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39mview([flattened_size] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(adj\u001b[39m.\u001b[39msize())[\u001b[39m3\u001b[39m:])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 29.81 GiB (GPU 1; 79.20 GiB total capacity; 779.04 MiB already allocated; 17.36 GiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "poison_adj_dense = to_dense_adj(poison_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = list(range(data.x.shape[0],poison_x.shape[0]))\n",
    "for nid in idx_attach:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = poison_adj_dense[0][nid].nonzero()\n",
    "    # bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(poison_x[u].unsqueeze(0),poison_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1370 is out of bounds for dimension 0 with size 36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m bkd_nids \u001b[39m=\u001b[39m induct_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m poison_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m nid \u001b[39min\u001b[39;00m idx_atk:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# polished_dr_test = copy.deepcopy(bkd_dr_test)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     polished_adj_nodes \u001b[39m=\u001b[39m induct_adj_dense[\u001b[39m0\u001b[39;49m][nid]\u001b[39m.\u001b[39mnonzero()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     bkd_nids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(poison_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],induct_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m polished_adj_nodes:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1370 is out of bounds for dimension 0 with size 36"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "induct_adj_dense = to_dense_adj(induct_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = induct_x.shape[0] - poison_x.shape[0]\n",
    "for nid in idx_atk:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "for nid in idx_test:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        # if(v in bkd_nids):\n",
    "        u = nid\n",
    "        print(nid,v)\n",
    "        print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_homophily(adj,x,device):\n",
    "    deg_vector = adj.sum(1)\n",
    "    deg_matrix = torch.diag(adj.sum(1)).to(device)\n",
    "    deg_matrix += torch.eye(len(adj)).to(device)\n",
    "    deg_inv_sqrt = deg_matrix.pow(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "    adj = torch.matmul(deg_inv_sqrt,adj)\n",
    "    adj = torch.matmul(adj,deg_inv_sqrt)\n",
    "    x_neg = adj @ x\n",
    "    node_sims = np.array([float(F.cosine_similarity(xn.unsqueeze(0),xx.unsqueeze(0))) for (xn,xx) in zip(x_neg,x)])   \n",
    "    # node_sims = np.array([torch.round(i,decimals=2) for i in node_sims])\n",
    "    # print(node_sims)\n",
    "    return node_sims\n",
    "bkd_graph_test_node_sims = calculate_graph_homophily(to_dense_adj(data.edge_index)[0].to(device),data.x.to(device),device)\n",
    "bkd_graph_train_node_sims = calculate_graph_homophily(to_dense_adj(poison_edge_index)[0].to(device),poison_x.to(device),device)\n",
    "clean_graph_node_sims = calculate_graph_homophily(to_dense_adj(induct_edge_index)[0].to(device),induct_x.to(device),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def to_percent(y,position):\n",
    "    return str(100*y)+\"%\"#这里可以用round（）函数设置取几位小数\n",
    "\n",
    "plt.hist(clean_graph_node_sims,bins=10,weights=[1./len(clean_graph_node_sims)]*len(clean_graph_node_sims),density=True, alpha=0.75, label='clean')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_test_node_sims),bins=20,weights=[1./len(bkd_graph_test_node_sims)]*len(bkd_graph_test_node_sims),density=True, alpha=0.75, label='poison')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_train_node_sims),bins=20,weights=[1./len(bkd_graph_train_node_sims)]*len(bkd_graph_train_node_sims),density=True, alpha=0.75, label='attack')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "# plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.png\")\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_test_edge_index = to_dense_adj(data.edge_index)[0].nonzero()\n",
    "trigger_trigger_edge_sims = []\n",
    "trigger_trigger_edge_index = []\n",
    "\n",
    "trigger_target_edge_sims = []\n",
    "trigger_target_edge_index = []\n",
    "\n",
    "normal_normal_edge_sims = []\n",
    "normal_normal_edge_index = []\n",
    "\n",
    "trigger_normal_edge_sims = []\n",
    "trigger_normal_edge_index = []\n",
    "\n",
    "target_target_edge_sims = []\n",
    "target_target_edge_index = []\n",
    "for (u,v) in bkd_test_edge_index:\n",
    "    if ((v,u) in trigger_trigger_edge_index) or ((u,v) in trigger_trigger_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_target_edge_index) or ((u,v) in trigger_target_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in normal_normal_edge_index) or ((u,v) in normal_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_normal_edge_index) or ((u,v) in trigger_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in target_target_edge_index) or ((u,v) in target_target_edge_index):\n",
    "        continue\n",
    "    \n",
    "    if (u in bkd_nids) and (v in bkd_nids):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_trigger_edge_sims.append(edge_sims)\n",
    "        trigger_trigger_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_atk)) or ((v in bkd_nids) and (u in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_target_edge_sims.append(edge_sims)\n",
    "        trigger_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    if (u in idx_test) and (v in idx_test):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        normal_normal_edge_sims.append(edge_sims)\n",
    "        normal_normal_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_test)) or ((v in bkd_nids) and (u in idx_test)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_normal_edge_sims.append(edge_sims)\n",
    "        trigger_normal_edge_index.append((u,v))\n",
    "\n",
    "    if ((u in idx_atk) and (v in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        target_target_edge_sims.append(edge_sims)\n",
    "        target_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch120': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ab847dfc59cee10fa08e4e9fed31787c275fa5742f67664facc345e6fad65e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
