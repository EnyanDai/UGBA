{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attack_method='Basic', cuda=True, dataset='ogbn-arxiv', debug=True, defense_mode='isolated', device_id=1, dis_weight=1, dropout=0.5, epochs=200, evaluate_mode='overall', hidden=64, homo_boost_thrd=0.5, homo_loss_weight=1, inner=1, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.35, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, test_model='GCN', thrd=0.5, train_lr=0.02, trigger_prob=0.5, trigger_size=3, trojan_epochs=200, use_vs_number=False, vs_number=0, vs_ratio=0.001, weight_decay=0.0005)\n",
      "Length of training set: 33868\n",
      "Training benign model Finished!\n",
      "Total time elapsed: 11.7324s\n",
      "Benign CA: 0.6619\n",
      "Length of training set: 33868\n",
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 3.6526167392730713\n",
      "acc_val: 0.1333\n",
      "Epoch 10, training loss: 2.751762628555298\n",
      "acc_val: 0.2825\n",
      "Epoch 20, training loss: 2.228600263595581\n",
      "acc_val: 0.4161\n",
      "Epoch 30, training loss: 1.9383784532546997\n",
      "acc_val: 0.5150\n",
      "Epoch 40, training loss: 1.7810544967651367\n",
      "acc_val: 0.5503\n",
      "Epoch 50, training loss: 1.6715518236160278\n",
      "acc_val: 0.5817\n",
      "Epoch 60, training loss: 1.6001300811767578\n",
      "acc_val: 0.5958\n",
      "Epoch 70, training loss: 1.5650560855865479\n",
      "acc_val: 0.6059\n",
      "Epoch 80, training loss: 1.5416816473007202\n",
      "acc_val: 0.6212\n",
      "Epoch 90, training loss: 1.5210376977920532\n",
      "acc_val: 0.6166\n",
      "Epoch 100, training loss: 1.4990050792694092\n",
      "acc_val: 0.6274\n",
      "Epoch 110, training loss: 1.4902468919754028\n",
      "acc_val: 0.6281\n",
      "Epoch 120, training loss: 1.4671518802642822\n",
      "acc_val: 0.6339\n",
      "Epoch 130, training loss: 1.4576902389526367\n",
      "acc_val: 0.6377\n",
      "Epoch 140, training loss: 1.451909065246582\n",
      "acc_val: 0.6370\n",
      "Epoch 150, training loss: 1.4479390382766724\n",
      "acc_val: 0.6368\n",
      "Epoch 160, training loss: 1.4317941665649414\n",
      "acc_val: 0.6414\n",
      "Epoch 170, training loss: 1.4298443794250488\n",
      "acc_val: 0.6407\n",
      "Epoch 180, training loss: 1.421897053718567\n",
      "acc_val: 0.6418\n",
      "Epoch 190, training loss: 1.425553798675537\n",
      "acc_val: 0.6439\n",
      "=== picking the best model according to the performance on validation ===\n",
      "Training encoder Finished!\n",
      "Total time elapsed: 12.7878s\n",
      "Encoder CA on clean test nodes: 0.6483\n",
      "running k-means on cuda:1..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 159it [09:43,  3.67s/it, center_shift=0.000017, iteration=159, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 28 28 ... 24  4  4]\n",
      "Epoch 0, loss_inner: 3.65077, loss_target: 3.44922, homo loss: 0.53180 \n",
      "acc_train_clean: 0.1599, ASR_train_attach: 0.0000, ASR_train_outter: 0.0000\n",
      "Epoch 10, loss_inner: 2.80903, loss_target: 2.81291, homo loss: 0.00043 \n",
      "acc_train_clean: 0.2830, ASR_train_attach: 0.0000, ASR_train_outter: 0.0000\n",
      "Epoch 20, loss_inner: 2.38171, loss_target: 2.33512, homo loss: 0.00025 \n",
      "acc_train_clean: 0.3591, ASR_train_attach: 0.5037, ASR_train_outter: 0.4498\n",
      "Epoch 30, loss_inner: 1.99055, loss_target: 1.94164, homo loss: 0.00052 \n",
      "acc_train_clean: 0.4892, ASR_train_attach: 0.9926, ASR_train_outter: 0.8717\n",
      "Epoch 40, loss_inner: 1.71890, loss_target: 1.68128, homo loss: 0.00026 \n",
      "acc_train_clean: 0.5495, ASR_train_attach: 1.0000, ASR_train_outter: 0.9474\n",
      "Epoch 50, loss_inner: 1.54396, loss_target: 1.51084, homo loss: 0.00010 \n",
      "acc_train_clean: 0.5837, ASR_train_attach: 1.0000, ASR_train_outter: 0.9768\n",
      "Epoch 60, loss_inner: 1.42736, loss_target: 1.40058, homo loss: 0.00003 \n",
      "acc_train_clean: 0.6048, ASR_train_attach: 1.0000, ASR_train_outter: 0.9691\n",
      "Epoch 70, loss_inner: 1.35361, loss_target: 1.33038, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6194, ASR_train_attach: 1.0000, ASR_train_outter: 0.9845\n",
      "Epoch 80, loss_inner: 1.30487, loss_target: 1.28398, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6288, ASR_train_attach: 1.0000, ASR_train_outter: 0.9861\n",
      "Epoch 90, loss_inner: 1.26824, loss_target: 1.24839, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6370, ASR_train_attach: 1.0000, ASR_train_outter: 0.9876\n",
      "Epoch 100, loss_inner: 1.23829, loss_target: 1.21968, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6424, ASR_train_attach: 1.0000, ASR_train_outter: 0.9876\n",
      "Epoch 110, loss_inner: 1.21527, loss_target: 1.19944, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6485, ASR_train_attach: 1.0000, ASR_train_outter: 0.9845\n",
      "Epoch 120, loss_inner: 1.19795, loss_target: 1.18185, homo loss: 0.00000 \n",
      "acc_train_clean: 0.6524, ASR_train_attach: 1.0000, ASR_train_outter: 0.9892\n",
      "Epoch 130, loss_inner: 1.20171, loss_target: 1.19842, homo loss: 0.00012 \n",
      "acc_train_clean: 0.6510, ASR_train_attach: 1.0000, ASR_train_outter: 0.9954\n",
      "Epoch 140, loss_inner: 1.20092, loss_target: 1.21495, homo loss: 0.00013 \n",
      "acc_train_clean: 0.6509, ASR_train_attach: 1.0000, ASR_train_outter: 0.9954\n",
      "Epoch 150, loss_inner: 1.22615, loss_target: 1.27184, homo loss: 0.00048 \n",
      "acc_train_clean: 0.6546, ASR_train_attach: 0.4444, ASR_train_outter: 0.9985\n",
      "Epoch 160, loss_inner: 1.26315, loss_target: 1.36054, homo loss: 0.00661 \n",
      "acc_train_clean: 0.6478, ASR_train_attach: 1.0000, ASR_train_outter: 1.0000\n",
      "Epoch 170, loss_inner: 1.33709, loss_target: 1.49239, homo loss: 0.01349 \n",
      "acc_train_clean: 0.6455, ASR_train_attach: 1.0000, ASR_train_outter: 1.0000\n",
      "Epoch 180, loss_inner: 1.46075, loss_target: 1.46369, homo loss: 0.00749 \n",
      "acc_train_clean: 0.6353, ASR_train_attach: 1.0000, ASR_train_outter: 1.0000\n",
      "Epoch 190, loss_inner: 1.37647, loss_target: 1.63317, homo loss: 0.00655 \n",
      "acc_train_clean: 0.6330, ASR_train_attach: 1.0000, ASR_train_outter: 1.0000\n",
      "precent of left attach nodes: 1.000\n",
      "target class rate on Vs: 1.0000\n",
      "accuracy on clean test nodes: 0.6548\n",
      "ASR: 1.0000\n",
      "CA: 0.5995\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]: \n",
    "\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.GCN import GCN\n",
    "from models.GCN_Encoder import GCN_Encoder\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork,Reddit,Reddit2,Flickr,Yelp,PPI\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated,select_target_nodes\n",
    "import help_funcs\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='ogbn-arxiv', \n",
    "                    help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--train_lr', type=float, default=0.02,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=64,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=200, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "# backdoor setting\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--use_vs_number', action='store_true', default=False,\n",
    "                    help=\"if use detailed number to decide Vs\")\n",
    "parser.add_argument('--vs_ratio', type=float, default=0.001,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--vs_number', type=int, default=0,\n",
    "                    help=\"number of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"isolated\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.35,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--attack_method', type=str, default='Basic',\n",
    "                    choices=['Rand_Gene','Rand_Samp','Basic','None'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--trigger_prob', type=float, default=0.5,\n",
    "                    help=\"The probability to generate the trigger's edges in random method\")\n",
    "parser.add_argument('--selection_method', type=str, default='cluster_degree',\n",
    "                    choices=['loss','conf','cluster','none','cluster_degree'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "parser.add_argument('--evaluate_mode', type=str, default='overall',\n",
    "                    choices=['overall','1by1'],\n",
    "                    help='Model used to attack')\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=1,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "# device2 = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id+1))\n",
    "\n",
    "# np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "    #  random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(args.seed)\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "np.random.seed(11) # fix the random seed is important\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'PPI'):\n",
    "    dataset = PPI(root='./data/PPI/', \n",
    "                split='train', transform=None)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit2(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "elif(args.dataset == 'Yelp'):\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = Yelp(root='./data/Yelp/')\n",
    "    # idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "if(args.dataset == 'ogbn-arxiv'):\n",
    "    nNode = data.x.shape[0]\n",
    "    setattr(data,'train_mask',torch.zeros(nNode, dtype=torch.bool).to(device))\n",
    "    # dataset[0].train_mask = torch.zeros(nEdge, dtype=torch.bool).to(device)\n",
    "    data.val_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.test_mask = torch.zeros(nNode, dtype=torch.bool).to(device)\n",
    "    data.y = data.y.squeeze(1)\n",
    "# we build our own train test split \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(args,data,device)\n",
    "\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# In[6]: \n",
    "import os\n",
    "from models.backdoor import model_construct\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath)):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    # edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    benign_model = model_construct(args,args.model,data,device).to(device) \n",
    "    t_total = time.time()\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    # torch.save(benign_model, benign_modelpath)\n",
    "    # print(\"Benign model saved at {}\".format(benign_modelpath))\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, None, data.y,idx_clean_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "benign_model = benign_model.cpu()\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import Backdoor\n",
    "from heuristic_selection import obtain_attach_nodes, obtain_attach_nodes_by_cluster_degree, obtain_attach_nodes_by_cluster_gpu,obtain_attach_nodes_by_influential,obtain_attach_nodes_by_cluster,cluster_distance_selection,cluster_degree_selection\n",
    "\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "if(args.use_vs_number):\n",
    "    size = args.vs_number\n",
    "else:\n",
    "    size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(args,unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,train_edge_index,None,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    idx_attach = cluster_distance_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster_degree'):\n",
    "    idx_attach = cluster_degree_selection(args,data,idx_train,idx_val,idx_clean_test,unlabeled_idx,train_edge_index,size,device)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "\n",
    "# In[10]:\n",
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "if(args.attack_method == 'Basic'):\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "    # model.fit_rand(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned_rand(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "elif(args.attack_method == 'None'):\n",
    "    train_edge_weights = torch.ones([train_edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = data.x.clone(), train_edge_index.clone(), train_edge_weights, data.y.clone()\n",
    "# In[12]:\n",
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "# if(args.attack_method == 'none'):\n",
    "#     bkd_tn_nodes = idx_train\n",
    "print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "    .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "#%%\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "# test_model = test_model.cpu()\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "# poison_x, poison_edge_index, poison_edge_weights, poison_labels = poison_x.to(device2), poison_edge_index.to(device2), poison_edge_weights.to(device2), poison_labels.to(device2)\n",
    "# model.trojan = model.trojan.cpu()\n",
    "if(args.evaluate_mode == '1by1'):\n",
    "    if(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(idx_atk,poison_x,induct_edge_index,induct_edge_weights,data.y)\n",
    "        # induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "    \n",
    "    from torch_geometric.utils  import k_hop_subgraph\n",
    "    overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "    asr = 0\n",
    "    flip_asr = 0\n",
    "    flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "    for i, idx in enumerate(idx_atk):\n",
    "        idx=int(idx)\n",
    "        sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "        ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "        relabeled_node_idx = sub_mapping\n",
    "        sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "        # inject trigger on attack test nodes (idx_atk)'''\n",
    "        if(args.attack_method == 'Basic'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "        elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "            # induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,data.y[sub_induct_nodeset], full_data=True)\n",
    "            induct_x, induct_edge_index,induct_edge_weights = induct_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights\n",
    "        elif(args.attack_method == 'None'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights\n",
    "\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # # do pruning in test datas'''\n",
    "        print(induct_edge_index.shape,induct_edge_weights.shape,induct_x.shape)\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        # test_model = test_model.to(device)\n",
    "        output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "        print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        asr += train_attach_rate\n",
    "        if(data.y[idx] != args.target_class):\n",
    "            flip_asr += train_attach_rate\n",
    "        # ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        # print(\"CA: {:.4f}\".format(ca))\n",
    "    asr = asr/(idx_atk.shape[0])\n",
    "    flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "    print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "    print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "elif(args.evaluate_mode == 'overall'):\n",
    "    # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "    if(args.attack_method == 'Basic'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "    elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(idx_atk,poison_x,induct_edge_index,induct_edge_weights,data.y)\n",
    "    elif(args.attack_method == 'None'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = poison_x,induct_edge_index,induct_edge_weights\n",
    "\n",
    "    induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "    # do pruning in test datas'''\n",
    "    if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "        induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "    # attack evaluation\n",
    "\n",
    "    # test_model = test_model.to(device)\n",
    "    output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "    print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "    ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "# if(args.attack_method == 'none'):\n",
    "#     bkd_tn_nodes = idx_train\n",
    "print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "    .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "#%%\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=args.epochs,verbose=False)\n",
    "\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "# test_model = test_model.cpu()\n",
    "\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "# poison_x, poison_edge_index, poison_edge_weights, poison_labels = poison_x.to(device2), poison_edge_index.to(device2), poison_edge_weights.to(device2), poison_labels.to(device2)\n",
    "# model.trojan = model.trojan.cpu()\n",
    "if(args.evaluate_mode == '1by1'):\n",
    "    from torch_geometric.utils  import k_hop_subgraph\n",
    "    overall_induct_edge_index, overall_induct_edge_weights = induct_edge_index.clone(),induct_edge_weights.clone()\n",
    "    asr = 0\n",
    "    flip_asr = 0\n",
    "    flip_idx_atk = idx_atk[(data.y[idx_atk] != args.target_class).nonzero().flatten()]\n",
    "    for i, idx in enumerate(idx_atk):\n",
    "        idx=int(idx)\n",
    "        sub_induct_nodeset, sub_induct_edge_index, sub_mapping, sub_edge_mask  = k_hop_subgraph(node_idx = [idx], num_hops = 2, edge_index = overall_induct_edge_index, relabel_nodes=True) # sub_mapping means the index of [idx] in sub)nodeset\n",
    "        ori_node_idx = sub_induct_nodeset[sub_mapping]\n",
    "        relabeled_node_idx = sub_mapping\n",
    "        sub_induct_edge_weights = torch.ones([sub_induct_edge_index.shape[1]]).to(device)\n",
    "        # inject trigger on attack test nodes (idx_atk)'''\n",
    "        if(args.attack_method == 'Basic'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,device)\n",
    "        elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(relabeled_node_idx,poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights,data.y[sub_induct_nodeset], full_data=True)\n",
    "        elif(args.attack_method == 'None'):\n",
    "            induct_x, induct_edge_index,induct_edge_weights = poison_x[sub_induct_nodeset],sub_induct_edge_index,sub_induct_edge_weights\n",
    "\n",
    "        induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "        # # do pruning in test datas'''\n",
    "        if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "            induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "        # attack evaluation\n",
    "\n",
    "        # test_model = test_model.to(device)\n",
    "        output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "        train_attach_rate = (output.argmax(dim=1)[relabeled_node_idx]==args.target_class).float().mean()\n",
    "        print(\"Node {}: {}, Origin Label: {}\".format(i, idx, data.y[idx]))\n",
    "        print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "        asr += train_attach_rate\n",
    "        if(data.y[idx] != args.target_class):\n",
    "            flip_asr += train_attach_rate\n",
    "        # ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "        # print(\"CA: {:.4f}\".format(ca))\n",
    "    asr = asr/(idx_atk.shape[0])\n",
    "    flip_asr = flip_asr/(flip_idx_atk.shape[0])\n",
    "    print(\"Overall ASR: {:.4f}\".format(asr))\n",
    "    print(\"Flip ASR: {:.4f}/{} nodes\".format(flip_asr,flip_idx_atk.shape[0]))\n",
    "elif(args.evaluate_mode == 'overall'):\n",
    "    # %% inject trigger on attack test nodes (idx_atk)'''\n",
    "    if(args.attack_method == 'Basic'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights,device)\n",
    "    elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(idx_atk,poison_x,induct_edge_index,induct_edge_weights,data.y)\n",
    "    elif(args.attack_method == 'None'):\n",
    "        induct_x, induct_edge_index,induct_edge_weights = poison_x,induct_edge_index,induct_edge_weights\n",
    "\n",
    "    induct_x, induct_edge_index,induct_edge_weights = induct_x.clone().detach(), induct_edge_index.clone().detach(),induct_edge_weights.clone().detach()\n",
    "    # do pruning in test datas'''\n",
    "    if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "        induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "    # attack evaluation\n",
    "\n",
    "    # test_model = test_model.to(device)\n",
    "    output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "    train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "    print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "    ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "    print(\"CA: {:.4f}\".format(ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign model saved at ./modelpath/GCN_Reddit2_benign.pth\n",
      "Benign CA: 0.7006\n"
     ]
    }
   ],
   "source": [
    "torch.save(benign_model, benign_modelpath)\n",
    "print(\"Benign model saved at {}\".format(benign_modelpath))\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, None, data.y,idx_clean_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "benign_model = benign_model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# splitted_idx = data.get_idx_split()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m idx_train, idx_val, idx_test \u001b[39m=\u001b[39m split_idx[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m graph, labels \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./dataset/')\n",
    "\n",
    "split_idx = dataset.get_idx_split() \n",
    "# splitted_idx = data.get_idx_split()\n",
    "idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n",
    "# train_loader = DataLoader(dataset[split_idx['train']])\n",
    "# valid_loader = DataLoader(dataset[split_idx['valid']])\n",
    "# test_loader = DataLoader(dataset[split_idx['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "# if args.dataset in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "dataset = Planetoid(root='./data/', split=\"random\", num_train_per_class=80, num_val=400, num_test=1000, \\\n",
    "                    name=args.dataset,transform=None)\n",
    "# dataset = Reddit(root='./data/', transform=transform, pre_transform=None)\n",
    "# dataset = classFlickr(root='./data/', transform=transform, pre_transform=None)\n",
    "\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "# get the overall edge index of the graph\n",
    "data.edge_index = to_undirected(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  mask the test nodes\n",
    "from utils import subgraph\n",
    "# get the edge index used for training (except from test nodes) and \n",
    "train_edge_index,train_edge_weights, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "idx_train =data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_test = data.test_mask.nonzero().flatten()\n",
    "# val_mask = node_idx[data.val_mask]\n",
    "# labels = data.y[torch.bitwise_not(data.test_mask)]\n",
    "# features = data.x[torch.bitwise_not(data.test_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028799998253884154\n"
     ]
    }
   ],
   "source": [
    "e = data.edge_index.shape[1]\n",
    "t = data.x.shape[0]\n",
    "p = 2*e/(t*(t-1))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "from models.SAGE import GraphSage\n",
    "def model_construct(args,model_name,data):\n",
    "    if (model_name == 'GCN'):\n",
    "        model = GCN(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GAT'):\n",
    "        model = GAT(nfeat=data.x.shape[1], \n",
    "                    nhid=args.hidden, \n",
    "                    nclass=int(data.y.max()+1), \n",
    "                    heads=8,\n",
    "                    dropout=args.dropout, \n",
    "                    lr=args.lr, \n",
    "                    weight_decay=args.weight_decay, \n",
    "                    device=device)\n",
    "    elif(model_name == 'GraphSage'):\n",
    "        model = GraphSage(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GCN_Encoder'):\n",
    "        model = GCN_Encoder(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train benign model\n",
    "'''\n",
    "import os\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath) and args.load_benign_model):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    benign_model = model_construct(args,args.model,data).to(device) \n",
    "    t_total = time.time()\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    torch.save(benign_model, benign_modelpath)\n",
    "    print(\"Benign model saved at {}\".format(benign_modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign CA: 0.8410\n",
      "Benign CA on clean test nodes: 0.8250\n"
     ]
    }
   ],
   "source": [
    "benign_output = benign_model(data.x, data.edge_index, edge_weights)\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "atk_test_nodes, clean_test_nodes,poi_train_nodes = select_target_nodes(args,args.seed,benign_model,data.x, data.edge_index, edge_weights,data.y,idx_val,idx_test)\n",
    "clean_test_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "print(\"Benign CA on clean test nodes: {:.4f}\".format(clean_test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n",
      "Encoder CA: 0.8380\n",
      "Encoder CA on clean test nodes: 0.8000\n",
      "[4 4 3 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import obtain_attach_nodes,Backdoor,obtain_attach_nodes_by_influential,obtain_attach_nodes_by_cluster\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "# poison nodes' size\n",
    "size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,train_edge_index,train_edge_weights,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    # construct GCN encoder\n",
    "    encoder_modelpath = './modelpath/{}_{}_benign.pth'.format('GCN_Encoder', args.dataset)\n",
    "    if(os.path.exists(encoder_modelpath) and args.load_benign_model):\n",
    "        # load existing benign model\n",
    "        gcn_encoder = torch.load(encoder_modelpath)\n",
    "        gcn_encoder = gcn_encoder.to(device)\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "    else:\n",
    "        gcn_encoder = model_construct(args,'GCN_Encoder',data).to(device) \n",
    "        t_total = time.time()\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "        gcn_encoder.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "        print(\"Training encoder Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        # Save trained model\n",
    "        torch.save(gcn_encoder, encoder_modelpath)\n",
    "        print(\"Encoder saved at {}\".format(encoder_modelpath))\n",
    "    # test gcn encoder \n",
    "    encoder_benign_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "    print(\"Encoder CA: {:.4f}\".format(encoder_benign_ca))\n",
    "    encoder_clean_test_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "    print(\"Encoder CA on clean test nodes: {:.4f}\".format(encoder_clean_test_ca))\n",
    "    # from sklearn import cluster\n",
    "    seen_node_idx = torch.concat([idx_train,unlabeled_idx])\n",
    "    nclass = np.unique(data.y.cpu().numpy()).shape[0]\n",
    "    # kmeans = cluster.KMeans(n_clusters=nclass,random_state=1)\n",
    "    # kmeans.fit(data.x[seen_node_idx].cpu().numpy())\n",
    "    # # unlabeled_idx.cpu().tolist()\n",
    "\n",
    "    # train_adj = to_dense_adj(train_edge_index,edge_attr=train_edge_weights)[0].cpu()\n",
    "    # train_adj = train_adj + train_adj @ train_adj\n",
    "    # train_adj = torch.where(train_adj>0, torch.tensor(1.0),\n",
    "    #                                             torch.tensor(0.0))\n",
    "    # train_x = train_adj @ data.x.cpu()\n",
    "    # new_train_edge_index, new_train_edge_weights= dense_to_sparse(train_adj)\n",
    "    # kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    # kmeans.fit(train_x.detach().cpu().numpy())\n",
    "    # idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),train_x,data.y,device,size)\n",
    "    # idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    \n",
    "    encoder_x = gcn_encoder.get_h(data.x, train_edge_index,train_edge_weights).clone().detach()\n",
    "    kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    kmeans.fit(encoder_x.detach().cpu().numpy())\n",
    "    idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),encoder_x,data.y,device,size)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor(1.9393, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(20.4000, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 0, training loss: 1.9392701387405396\n",
      "acc_train_clean: 0.1768, acc_train_attach: 0.0588\n",
      "tensor(1.7866, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.8495, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.6262, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(13.2455, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.4436, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(11.8224, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.3043, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(10.5700, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.1444, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.4848, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.0180, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(8.4280, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.9117, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(6.9387, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.8150, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(5.7199, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.7289, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(4.5158, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.6040, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.5913, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 10, training loss: 0.6040130853652954\n",
      "acc_train_clean: 0.9000, acc_train_attach: 0.7059\n",
      "tensor(0.5469, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.7172, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.5104, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.5027, device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "print(args.epochs)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "poison_x = model.poison_x.data\n",
    "poison_edge_index = model.poison_edge_index.data\n",
    "poison_edge_weights = model.poison_edge_weights.data\n",
    "poison_labels = model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "577\n",
      "4306 2880\n",
      "{1569, 2081, 1699, 1251, 133, 2185, 843, 1775, 303, 305, 1780, 1814, 1782, 2143, 1243, 2046, 1791}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(torch.cat([idx_train,idx_attach])))\n",
    "print(len(bkd_tn_nodes))\n",
    "print(len(model.poison_edge_index.data[0]),len(poison_edge_index[0]))\n",
    "# print(idx_attach & bkd_tn_nodes)\n",
    "print(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9486966133117676\n",
      "acc_val: 0.3725\n",
      "Epoch 10, training loss: 0.5837284326553345\n",
      "acc_val: 0.7350\n",
      "Epoch 20, training loss: 0.22602947056293488\n",
      "acc_val: 0.7625\n",
      "Epoch 30, training loss: 0.126153364777565\n",
      "acc_val: 0.7625\n",
      "Epoch 40, training loss: 0.09417053312063217\n",
      "acc_val: 0.7625\n",
      "Epoch 50, training loss: 0.08149732649326324\n",
      "acc_val: 0.7525\n",
      "Epoch 60, training loss: 0.08437832444906235\n",
      "acc_val: 0.7425\n",
      "Epoch 70, training loss: 0.07754302769899368\n",
      "acc_val: 0.7500\n",
      "Epoch 80, training loss: 0.08388686925172806\n",
      "acc_val: 0.7575\n",
      "Epoch 90, training loss: 0.0656595528125763\n",
      "acc_val: 0.7525\n",
      "Epoch 100, training loss: 0.0655829906463623\n",
      "acc_val: 0.7475\n",
      "Epoch 110, training loss: 0.06835032254457474\n",
      "acc_val: 0.7600\n",
      "Epoch 120, training loss: 0.060565438121557236\n",
      "acc_val: 0.7525\n",
      "Epoch 130, training loss: 0.07300901412963867\n",
      "acc_val: 0.7550\n",
      "Epoch 140, training loss: 0.05255601555109024\n",
      "acc_val: 0.7500\n",
      "Epoch 150, training loss: 0.05699390545487404\n",
      "acc_val: 0.7500\n",
      "Epoch 160, training loss: 0.0540340282022953\n",
      "acc_val: 0.7450\n",
      "Epoch 170, training loss: 0.060852691531181335\n",
      "acc_val: 0.7475\n",
      "Epoch 180, training loss: 0.05091455578804016\n",
      "acc_val: 0.7500\n",
      "Epoch 190, training loss: 0.064822718501091\n",
      "acc_val: 0.7450\n",
      "=== picking the best model according to the performance on validation ===\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "test_model = model_construct(args,args.test_model,data).to(device) \n",
    "if(args.test_model == 'GraphSage' or args.test_model == 'GAT'):\n",
    "    poison_adj = to_dense_adj(poison_edge_index, edge_attr=poison_edge_weights)\n",
    "    poison_edge_index, poison_edge_weights = dense_to_sparse(poison_adj)\n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=200,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target class rate on Vs: 0.8824\n",
      "accuracy on clean test nodes: 0.8050\n",
      "ASR: 0.7700\n",
      "CA: 0.7550\n"
     ]
    }
   ],
   "source": [
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "# idx_test = data.test_mask.nonzero().flatten()[:200]\n",
    "# idx_test = list(set(data.test_mask.nonzero().flatten().tolist()) - set(atk_test_nodes))\n",
    "# idx_atk = data.test_mask.nonzero().flatten()[200:].tolist()\n",
    "# yt_nids = [nid for nid in idx_atk if data.y.tolist()==args.target_class] \n",
    "# yx_nids = torch.LongTensor(list(set(idx_atk) - set(yt_nids))).to(device)\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[atk_test_nodes] = args.target_class\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "'''clean accuracy of clean test nodes before injecting triggers to the attack test nodes'''\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "'''inject trigger on attack test nodes (idx_atk)'''\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(atk_test_nodes,poison_x,induct_edge_index,induct_edge_weights)\n",
    "'''do pruning in test datas'''\n",
    "if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "    induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "'''attack evaluation'''\n",
    "asr = test_model.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,atk_test_nodes)\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "print(\"ASR: {:.4f}\".format(asr))\n",
    "print(\"CA: {:.4f}\".format(ca))\n",
    "# output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "# train_attach_rate = (output.argmax(dim=1)[atk_test_nodes]==args.target_class).float().mean()\n",
    "# print(\"ASR: {:.4f}\".format(train_attach_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "sp_induct_x = help_funcs.normalize(sp.csr_matrix(induct_x.cpu().detach().numpy()))\n",
    "sp_induct_adj = help_funcs.normalize_adj(sp.csr_matrix(to_dense_adj(induct_edge_index)[0].cpu().detach().numpy()))\n",
    "induct_x = torch.FloatTensor(np.array(sp_induct_x.todense())).to(device)\n",
    "induct_adj = torch.FloatTensor(np.array(sp_induct_adj.todense())).to(device)\n",
    "induct_edge_index,induct_edge_weights = dense_to_sparse(induct_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[yx_nids]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "clean_acc = gcn.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_test)\n",
    "asr = gcn.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,idx_atk)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"ASR1: {:.4f}\".format(asr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_gcn = GCN(nfeat=data.x.shape[1],\\\n",
    "            nhid=args.hidden,\\\n",
    "            nclass= int(data.y.max()+1),\\\n",
    "            dropout=args.dropout,\\\n",
    "            lr=args.lr,\\\n",
    "            weight_decay=args.weight_decay,\\\n",
    "            device=device).to(device)\n",
    "#%%\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[idx_atk] = args.target_class\n",
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "benign_gcn.fit(data.x, data.edge_index, edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "benign_output = benign_gcn(data.x, data.edge_index, edge_weights)\n",
    "benign4poison_output = benign_gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "benign_ca = (benign_output.argmax(dim=1)[idx_test]==data.y[idx_test]).float().mean()\n",
    "benign4poison_ca = (benign4poison_output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"BenignCA: {:.4f}\".format(benign_ca))\n",
    "print(\"Benign for poisoning CA: {:.4f}\".format(benign4poison_ca))\n",
    "print((benign_output.argmax(dim=1)[yx_nids]==args.target_class).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_labels = data.y.clone()\n",
    "idx_atk = obtain_attach_nodes(data.test_mask.nonzero().flatten(), 200)\n",
    "can_test_nodes = torch.LongTensor(list(set(data.test_mask.nonzero().flatten()) - set(idx_atk))).to(device)\n",
    "idx_test = obtain_attach_nodes(can_test_nodes,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,data.x,data.edge_index,edge_weights)\n",
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "test_asr= (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(test_asr))\n",
    "test_ca = (output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"CA: {:.4f}\".format(test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 29.81 GiB (GPU 1; 79.20 GiB total capacity; 779.04 MiB already allocated; 17.36 GiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_dense_adj\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m poison_adj_dense \u001b[39m=\u001b[39m to_dense_adj(poison_edge_index)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39medge_sim_analysis\u001b[39m(edge_index, features):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     sims \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/torch_geometric/utils/to_dense_adj.py:48\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes)\u001b[0m\n\u001b[1;32m     46\u001b[0m size \u001b[39m=\u001b[39m [batch_size, max_num_nodes, max_num_nodes]\n\u001b[1;32m     47\u001b[0m size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(edge_attr\u001b[39m.\u001b[39msize())[\u001b[39m1\u001b[39m:]\n\u001b[0;32m---> 48\u001b[0m adj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(size, dtype\u001b[39m=\u001b[39;49medge_attr\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49medge_index\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     50\u001b[0m flattened_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m max_num_nodes \u001b[39m*\u001b[39m max_num_nodes\n\u001b[1;32m     51\u001b[0m adj \u001b[39m=\u001b[39m adj\u001b[39m.\u001b[39mview([flattened_size] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(adj\u001b[39m.\u001b[39msize())[\u001b[39m3\u001b[39m:])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 29.81 GiB (GPU 1; 79.20 GiB total capacity; 779.04 MiB already allocated; 17.36 GiB free; 1.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "poison_adj_dense = to_dense_adj(poison_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = list(range(data.x.shape[0],poison_x.shape[0]))\n",
    "for nid in idx_attach:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = poison_adj_dense[0][nid].nonzero()\n",
    "    # bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(poison_x[u].unsqueeze(0),poison_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1370 is out of bounds for dimension 0 with size 36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m bkd_nids \u001b[39m=\u001b[39m induct_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m poison_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m nid \u001b[39min\u001b[39;00m idx_atk:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# polished_dr_test = copy.deepcopy(bkd_dr_test)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     polished_adj_nodes \u001b[39m=\u001b[39m induct_adj_dense[\u001b[39m0\u001b[39;49m][nid]\u001b[39m.\u001b[39mnonzero()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     bkd_nids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(poison_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],induct_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m polished_adj_nodes:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1370 is out of bounds for dimension 0 with size 36"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "induct_adj_dense = to_dense_adj(induct_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = induct_x.shape[0] - poison_x.shape[0]\n",
    "for nid in idx_atk:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "for nid in idx_test:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        # if(v in bkd_nids):\n",
    "        u = nid\n",
    "        print(nid,v)\n",
    "        print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_homophily(adj,x,device):\n",
    "    deg_vector = adj.sum(1)\n",
    "    deg_matrix = torch.diag(adj.sum(1)).to(device)\n",
    "    deg_matrix += torch.eye(len(adj)).to(device)\n",
    "    deg_inv_sqrt = deg_matrix.pow(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "    adj = torch.matmul(deg_inv_sqrt,adj)\n",
    "    adj = torch.matmul(adj,deg_inv_sqrt)\n",
    "    x_neg = adj @ x\n",
    "    node_sims = np.array([float(F.cosine_similarity(xn.unsqueeze(0),xx.unsqueeze(0))) for (xn,xx) in zip(x_neg,x)])   \n",
    "    # node_sims = np.array([torch.round(i,decimals=2) for i in node_sims])\n",
    "    # print(node_sims)\n",
    "    return node_sims\n",
    "bkd_graph_test_node_sims = calculate_graph_homophily(to_dense_adj(data.edge_index)[0].to(device),data.x.to(device),device)\n",
    "bkd_graph_train_node_sims = calculate_graph_homophily(to_dense_adj(poison_edge_index)[0].to(device),poison_x.to(device),device)\n",
    "clean_graph_node_sims = calculate_graph_homophily(to_dense_adj(induct_edge_index)[0].to(device),induct_x.to(device),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def to_percent(y,position):\n",
    "    return str(100*y)+\"%\"#这里可以用round（）函数设置取几位小数\n",
    "\n",
    "plt.hist(clean_graph_node_sims,bins=10,weights=[1./len(clean_graph_node_sims)]*len(clean_graph_node_sims),density=True, alpha=0.75, label='clean')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_test_node_sims),bins=20,weights=[1./len(bkd_graph_test_node_sims)]*len(bkd_graph_test_node_sims),density=True, alpha=0.75, label='poison')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_train_node_sims),bins=20,weights=[1./len(bkd_graph_train_node_sims)]*len(bkd_graph_train_node_sims),density=True, alpha=0.75, label='attack')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "# plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.png\")\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_test_edge_index = to_dense_adj(data.edge_index)[0].nonzero()\n",
    "trigger_trigger_edge_sims = []\n",
    "trigger_trigger_edge_index = []\n",
    "\n",
    "trigger_target_edge_sims = []\n",
    "trigger_target_edge_index = []\n",
    "\n",
    "normal_normal_edge_sims = []\n",
    "normal_normal_edge_index = []\n",
    "\n",
    "trigger_normal_edge_sims = []\n",
    "trigger_normal_edge_index = []\n",
    "\n",
    "target_target_edge_sims = []\n",
    "target_target_edge_index = []\n",
    "for (u,v) in bkd_test_edge_index:\n",
    "    if ((v,u) in trigger_trigger_edge_index) or ((u,v) in trigger_trigger_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_target_edge_index) or ((u,v) in trigger_target_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in normal_normal_edge_index) or ((u,v) in normal_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_normal_edge_index) or ((u,v) in trigger_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in target_target_edge_index) or ((u,v) in target_target_edge_index):\n",
    "        continue\n",
    "    \n",
    "    if (u in bkd_nids) and (v in bkd_nids):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_trigger_edge_sims.append(edge_sims)\n",
    "        trigger_trigger_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_atk)) or ((v in bkd_nids) and (u in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_target_edge_sims.append(edge_sims)\n",
    "        trigger_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    if (u in idx_test) and (v in idx_test):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        normal_normal_edge_sims.append(edge_sims)\n",
    "        normal_normal_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_test)) or ((v in bkd_nids) and (u in idx_test)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_normal_edge_sims.append(edge_sims)\n",
    "        trigger_normal_edge_index.append((u,v))\n",
    "\n",
    "    if ((u in idx_atk) and (v in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        target_target_edge_sims.append(edge_sims)\n",
    "        target_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
