{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(clean_test_nodes_num=200, cuda=True, dataset='cora', debug=True, defense_mode='isolate', device_id=2, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.6, homo_loss_weight=1, load_benign_model=True, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.1, seed=10, selection_method='conf', target_class=0, target_test_nodes_num=200, test_model='GCN', thrd=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.GCN import GCN\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork,Reddit\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated,select_target_nodes\n",
    "import help_funcs\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='cora', help='Dataset',\n",
    "                    choices=['cora','citeseer','pubmed'])\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "# parser.add_argument('--trigger_size', type=int, default=3,\n",
    "#                     help='tirgger_size')\n",
    "# parser.add_argument('--vs_ratio', type=float, default=0.01)\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=200, help='Number of epochs to train trigger generator.')\n",
    "\n",
    "parser.add_argument('--load_benign_model', action='store_true', default=True,\n",
    "                    help='Loading benign model if exists.')\n",
    "# backdoor setting\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--vs_ratio', type=float, default=0.01,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "parser.add_argument('--target_test_nodes_num', type=float, default=200,\n",
    "                    help=\"the number of of test nodes attached with 1 (independent) trigger, which is corretly classified and not belong to the target class\")\n",
    "parser.add_argument('--clean_test_nodes_num', type=float, default=200,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"isolate\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.1,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.6,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--selection_method', type=str, default='conf',\n",
    "                    choices=['loss','conf','cluster','none'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=2,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "# args = parser.parse_known_args()[0]\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda:1\" if args.cuda else \"cpu\")\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "# if args.dataset in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "dataset = Planetoid(root='./data/', split=\"random\", num_train_per_class=80, num_val=400, num_test=1000, \\\n",
    "                    name=args.dataset,transform=None)\n",
    "# dataset = Reddit(root='./data/', transform=transform, pre_transform=None)\n",
    "# dataset = classFlickr(root='./data/', transform=transform, pre_transform=None)\n",
    "\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "# get the overall edge index of the graph\n",
    "data.edge_index = to_undirected(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  mask the test nodes\n",
    "from utils import subgraph\n",
    "# get the edge index used for training (except from test nodes) and \n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "idx_train =data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_test = data.test_mask.nonzero().flatten()\n",
    "# val_mask = node_idx[data.val_mask]\n",
    "# labels = data.y[torch.bitwise_not(data.test_mask)]\n",
    "# features = data.x[torch.bitwise_not(data.test_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "from models.SAGE import GraphSage\n",
    "def model_construct(args,model_name,data):\n",
    "    if (model_name == 'GCN'):\n",
    "        model = GCN(nfeat=data.x.shape[1],\\\n",
    "                    nhid=args.hidden,\\\n",
    "                    nclass= int(data.y.max()+1),\\\n",
    "                    dropout=args.dropout,\\\n",
    "                    lr=args.lr,\\\n",
    "                    weight_decay=args.weight_decay,\\\n",
    "                    device=device)\n",
    "    elif(model_name == 'GAT'):\n",
    "        model = GAT(nfeat=data.x.shape[1], \n",
    "                    nhid=args.hidden, \n",
    "                    nclass=int(data.y.max()+1), \n",
    "                    heads=8,\n",
    "                    dropout=args.dropout, \n",
    "                    lr=args.lr, \n",
    "                    weight_decay=args.weight_decay, \n",
    "                    device=device)\n",
    "    elif(model_name == 'GraphSage'):\n",
    "        model = GraphSage(nfeat=data.x.shape[1],\\\n",
    "                    nhid=args.hidden,\\\n",
    "                    nclass= int(data.y.max()+1),\\\n",
    "                    dropout=args.dropout,\\\n",
    "                    lr=args.lr,\\\n",
    "                    weight_decay=args.weight_decay,\\\n",
    "                    device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train benign model\n",
    "'''\n",
    "import os\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath) and args.load_benign_model):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    # benign_model = GCN(nfeat=data.x.shape[1],\\\n",
    "    #             nhid=args.hidden,\\\n",
    "    #             nclass= int(data.y.max()+1),\\\n",
    "    #             dropout=args.dropout,\\\n",
    "    #             lr=args.lr,\\\n",
    "    #             weight_decay=args.weight_decay,\\\n",
    "    #             device=device).to(device)\n",
    "    benign_model = model_construct(args,args.model,data).to(device) \n",
    "    t_total = time.time()\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, data.edge_index, edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    torch.save(benign_model, benign_modelpath)\n",
    "    print(\"Benign model saved at {}\".format(benign_modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign CA: 0.8360\n",
      "Benign CA on clean test nodes: 0.7800\n"
     ]
    }
   ],
   "source": [
    "benign_output = benign_model(data.x, data.edge_index, edge_weights)\n",
    "# benign4poison_output = benign_gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "# benign_ca = (benign_output.argmax(dim=1)[idx_test]==data.y[idx_test]).float().mean()\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "# benign4poison_ca = (benign4poison_output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "atk_test_nodes, clean_test_nodes,poi_train_nodes = select_target_nodes(args,args.seed,benign_model,data.x, data.edge_index, edge_weights,data.y,idx_val,idx_test)\n",
    "clean_test_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "print(\"Benign CA on clean test nodes: {:.4f}\".format(clean_test_ca))\n",
    "# print(\"Benign for poisoning CA: {:.4f}\".format(benign4poison_ca))\n",
    "# print((benign_output.argmax(dim=1)[yx_nids]==args.target_class).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "from models.backdoor import obtain_attach_nodes,Backdoor,obtain_attach_nodes_by_influential\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "# poison nodes' size\n",
    "size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "print(len(data.test_mask))\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,data.edge_index,edge_weights,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor(1.9394, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(21.6000, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 0, training loss: 1.9393671751022339\n",
      "acc_train_clean: 0.1750, acc_train_attach: 0.1111\n",
      "tensor(1.7808, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(10.7935, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.6231, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(14.1113, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.4406, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(12.2913, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.2995, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(11.2433, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.1433, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(10.0998, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.0243, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(8.3671, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.9191, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(6.3128, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.8205, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(5.0051, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.7340, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.8164, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.6165, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.2053, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 10, training loss: 0.616539478302002\n",
      "acc_train_clean: 0.8964, acc_train_attach: 0.5556\n",
      "tensor(0.5580, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.2123, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.5237, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.0482, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.4564, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.7229, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3995, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.0071, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3669, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(1.4092, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3575, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.9234, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3382, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.6071, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3013, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.5839, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.3005, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.3914, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2749, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.3431, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 20, training loss: 0.27494144439697266\n",
      "acc_train_clean: 0.9339, acc_train_attach: 0.7222\n",
      "tensor(0.2736, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.3996, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2629, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1209, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2350, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2656, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2294, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1324, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2041, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2279, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2013, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.4873, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.2069, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0646, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1677, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1588, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1741, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.4016, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1837, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0193, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 30, training loss: 0.18371622264385223\n",
      "acc_train_clean: 0.9571, acc_train_attach: 0.6667\n",
      "tensor(0.1682, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0722, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1754, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2469, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1646, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0818, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1448, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1556, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0303, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1446, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0945, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1640, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1328, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0707, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1337, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1393, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1449, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0230, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 40, training loss: 0.14492398500442505\n",
      "acc_train_clean: 0.9786, acc_train_attach: 0.6111\n",
      "tensor(0.1362, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1576, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1168, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0153, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1360, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0777, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1175, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2409, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1304, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1681, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1119, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1145, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0223, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1098, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0184, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0989, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1132, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.3478, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 50, training loss: 0.11317496746778488\n",
      "acc_train_clean: 0.9839, acc_train_attach: 0.7222\n",
      "tensor(0.1025, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0025, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1112, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0055, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1118, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0201, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0942, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0571, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1043, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1133, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0950, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1017, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0898, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0939, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1093, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0661, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 60, training loss: 0.10932702571153641\n",
      "acc_train_clean: 0.9768, acc_train_attach: 0.7778\n",
      "tensor(0.1019, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1045, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1630, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0955, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0961, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0157, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1026, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0416, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0996, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0098, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1069, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0908, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1015, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0479, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0976, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 70, training loss: 0.0975746363401413\n",
      "acc_train_clean: 0.9839, acc_train_attach: 0.8889\n",
      "tensor(0.0932, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0745, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0900, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0368, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0899, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0345, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0864, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0864, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0895, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0010, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0934, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.1076, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0965, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0855, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 80, training loss: 0.08549340814352036\n",
      "acc_train_clean: 0.9875, acc_train_attach: 0.9444\n",
      "tensor(0.0864, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0902, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0949, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0967, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1300, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0908, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0118, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0835, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0886, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0025, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0811, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0769, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0772, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 90, training loss: 0.07721222937107086\n",
      "acc_train_clean: 0.9893, acc_train_attach: 1.0000\n",
      "tensor(0.0850, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0205, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0934, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0810, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0203, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0822, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0818, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0791, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0905, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0219, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0917, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0906, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0842, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0890, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 100, training loss: 0.0889972671866417\n",
      "acc_train_clean: 0.9875, acc_train_attach: 0.7778\n",
      "tensor(0.0842, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0220, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0795, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0839, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1447, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0884, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0867, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0777, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0757, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1786, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0761, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0798, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0536, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0828, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0010, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0753, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 110, training loss: 0.07527866959571838\n",
      "acc_train_clean: 0.9857, acc_train_attach: 0.9444\n",
      "tensor(0.0781, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0233, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0717, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0535, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0693, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0220, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0792, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0714, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0695, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0814, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0787, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0137, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0759, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0706, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 120, training loss: 0.07057007402181625\n",
      "acc_train_clean: 0.9875, acc_train_attach: 0.9444\n",
      "tensor(0.0735, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0745, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0778, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0086, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0725, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1367, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0758, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0722, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0696, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0118, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0806, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0740, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0681, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 130, training loss: 0.06805729866027832\n",
      "acc_train_clean: 0.9911, acc_train_attach: 0.9444\n",
      "tensor(0.0687, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0981, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0649, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0787, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0687, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0153, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0799, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0676, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0779, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0756, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0760, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.1515, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0633, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0714, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 140, training loss: 0.07139362394809723\n",
      "acc_train_clean: 0.9911, acc_train_attach: 0.8889\n",
      "tensor(0.0671, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0685, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0701, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0690, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0321, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0628, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0716, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0681, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0981, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0543, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0539, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0732, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0839, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0754, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0710, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0282, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 150, training loss: 0.0710211768746376\n",
      "acc_train_clean: 0.9911, acc_train_attach: 0.8889\n",
      "tensor(0.0686, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0737, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0742, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2515, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0778, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0137, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0802, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0749, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0678, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0636, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0682, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0338, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0694, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 160, training loss: 0.06943463534116745\n",
      "acc_train_clean: 0.9875, acc_train_attach: 0.9444\n",
      "tensor(0.0656, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0692, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0558, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0546, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0565, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0639, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0025, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0713, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0137, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0713, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0739, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0258, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0565, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0644, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0645, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.2179, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 170, training loss: 0.06445524096488953\n",
      "acc_train_clean: 0.9875, acc_train_attach: 1.0000\n",
      "tensor(0.0661, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0585, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0181, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0574, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0118, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0687, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0629, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0641, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0760, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0704, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0678, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0025, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0634, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0453, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0760, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 180, training loss: 0.0760323628783226\n",
      "acc_train_clean: 0.9857, acc_train_attach: 0.8333\n",
      "tensor(0.0548, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0577, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0608, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0563, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0320, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0692, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0588, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0582, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0616, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0103, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0619, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0612, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0533, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0487, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 190, training loss: 0.05329020693898201\n",
      "acc_train_clean: 0.9929, acc_train_attach: 0.9444\n",
      "tensor(0.0602, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0588, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0629, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0729, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0650, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0727, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0385, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0641, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0.0245, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0593, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.0551, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(0., device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "print(args.epochs)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "poison_x = model.poison_x.data\n",
    "poison_edge_index = model.poison_edge_index.data\n",
    "poison_edge_weights = model.poison_edge_weights.data\n",
    "poison_labels = model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 1986],\n",
      "        [   9, 2614],\n",
      "        [  12, 1001],\n",
      "        ...,\n",
      "        [2685, 2452],\n",
      "        [2686, 1657],\n",
      "        [2691,  206]])\n"
     ]
    }
   ],
   "source": [
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "320\n",
      "4314 2882\n",
      "{106, 1835, 2479, 505, 26, 1307}\n"
     ]
    }
   ],
   "source": [
    "print(len(torch.cat([idx_train,idx_attach])))\n",
    "print(len(bkd_tn_nodes))\n",
    "print(len(model.poison_edge_index.data[0]),len(poison_edge_index[0]))\n",
    "# print(idx_attach & bkd_tn_nodes)\n",
    "print(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9510772228240967\n",
      "acc_val: 0.3625\n",
      "Epoch 10, training loss: 0.5053321123123169\n",
      "acc_val: 0.7000\n",
      "Epoch 20, training loss: 0.14754590392112732\n",
      "acc_val: 0.7175\n",
      "Epoch 30, training loss: 0.09236279129981995\n",
      "acc_val: 0.7250\n",
      "Epoch 40, training loss: 0.06816352903842926\n",
      "acc_val: 0.7200\n",
      "Epoch 50, training loss: 0.06020592898130417\n",
      "acc_val: 0.7150\n",
      "Epoch 60, training loss: 0.0682452842593193\n",
      "acc_val: 0.7175\n",
      "Epoch 70, training loss: 0.04516971856355667\n",
      "acc_val: 0.7200\n",
      "Epoch 80, training loss: 0.0528641939163208\n",
      "acc_val: 0.6975\n",
      "Epoch 90, training loss: 0.044221844524145126\n",
      "acc_val: 0.7175\n",
      "Epoch 100, training loss: 0.05050992965698242\n",
      "acc_val: 0.7100\n",
      "Epoch 110, training loss: 0.048176392912864685\n",
      "acc_val: 0.7100\n",
      "Epoch 120, training loss: 0.04732782393693924\n",
      "acc_val: 0.7125\n",
      "Epoch 130, training loss: 0.0622921884059906\n",
      "acc_val: 0.7225\n",
      "Epoch 140, training loss: 0.04177570715546608\n",
      "acc_val: 0.7150\n",
      "Epoch 150, training loss: 0.03020920418202877\n",
      "acc_val: 0.7150\n",
      "Epoch 160, training loss: 0.04248952493071556\n",
      "acc_val: 0.7125\n",
      "Epoch 170, training loss: 0.03699953109025955\n",
      "acc_val: 0.7125\n",
      "Epoch 180, training loss: 0.04649367183446884\n",
      "acc_val: 0.7075\n",
      "Epoch 190, training loss: 0.042248889803886414\n",
      "acc_val: 0.7225\n",
      "=== picking the best model according to the performance on validation ===\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "test_model = model_construct(args,args.test_model,data).to(device) \n",
    "if(args.test_model == 'GraphSage' or args.test_model == 'GAT'):\n",
    "    poison_adj = to_dense_adj(poison_edge_index, edge_attr=poison_edge_weights)\n",
    "    poison_edge_index, poison_edge_weights = dense_to_sparse(poison_adj)\n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=200,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target class rate on Vs: 0.5000\n",
      "accuracy on clean test nodes: 0.8000\n",
      "ASR: 0.5800\n",
      "CA: 0.7900\n"
     ]
    }
   ],
   "source": [
    "# gcn.eval()\n",
    "# model.eval()\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "\n",
    "# idx_test = data.test_mask.nonzero().flatten()[:200]\n",
    "# idx_test = list(set(data.test_mask.nonzero().flatten().tolist()) - set(atk_test_nodes))\n",
    "# idx_atk = data.test_mask.nonzero().flatten()[200:].tolist()\n",
    "# yt_nids = [nid for nid in idx_atk if data.y.tolist()==args.target_class] \n",
    "# yx_nids = torch.LongTensor(list(set(idx_atk) - set(yt_nids))).to(device)\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[atk_test_nodes] = args.target_class\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "'''clean accuracy of clean test nodes before injecting triggers to the attack test nodes'''\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "'''inject trigger on attack test nodes (idx_atk)'''\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(atk_test_nodes,poison_x,induct_edge_index,induct_edge_weights)\n",
    "'''do pruning in test datas'''\n",
    "if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "    induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "'''attack evaluation'''\n",
    "asr = test_model.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,atk_test_nodes)\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "print(\"ASR: {:.4f}\".format(asr))\n",
    "print(\"CA: {:.4f}\".format(ca))\n",
    "# output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "# train_attach_rate = (output.argmax(dim=1)[atk_test_nodes]==args.target_class).float().mean()\n",
    "# print(\"ASR: {:.4f}\".format(train_attach_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "sp_induct_x = help_funcs.normalize(sp.csr_matrix(induct_x.cpu().detach().numpy()))\n",
    "sp_induct_adj = help_funcs.normalize_adj(sp.csr_matrix(to_dense_adj(induct_edge_index)[0].cpu().detach().numpy()))\n",
    "induct_x = torch.FloatTensor(np.array(sp_induct_x.todense())).to(device)\n",
    "induct_adj = torch.FloatTensor(np.array(sp_induct_adj.todense())).to(device)\n",
    "induct_edge_index,induct_edge_weights = dense_to_sparse(induct_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[yx_nids]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "clean_acc = gcn.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_test)\n",
    "asr = gcn.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,idx_atk)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"ASR1: {:.4f}\".format(asr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_gcn = GCN(nfeat=data.x.shape[1],\\\n",
    "            nhid=args.hidden,\\\n",
    "            nclass= int(data.y.max()+1),\\\n",
    "            dropout=args.dropout,\\\n",
    "            lr=args.lr,\\\n",
    "            weight_decay=args.weight_decay,\\\n",
    "            device=device).to(device)\n",
    "#%%\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[idx_atk] = args.target_class\n",
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "benign_gcn.fit(data.x, data.edge_index, edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "benign_output = benign_gcn(data.x, data.edge_index, edge_weights)\n",
    "benign4poison_output = benign_gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "benign_ca = (benign_output.argmax(dim=1)[idx_test]==data.y[idx_test]).float().mean()\n",
    "benign4poison_ca = (benign4poison_output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"BenignCA: {:.4f}\".format(benign_ca))\n",
    "print(\"Benign for poisoning CA: {:.4f}\".format(benign4poison_ca))\n",
    "print((benign_output.argmax(dim=1)[yx_nids]==args.target_class).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_labels = data.y.clone()\n",
    "idx_atk = obtain_attach_nodes(data.test_mask.nonzero().flatten(), 200)\n",
    "can_test_nodes = torch.LongTensor(list(set(data.test_mask.nonzero().flatten()) - set(idx_atk))).to(device)\n",
    "idx_test = obtain_attach_nodes(can_test_nodes,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,data.x,data.edge_index,edge_weights)\n",
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "test_asr= (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(test_asr))\n",
    "test_ca = (output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"CA: {:.4f}\".format(test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "poison_adj_dense = to_dense_adj(poison_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = list(range(data.x.shape[0],poison_x.shape[0]))\n",
    "for nid in idx_attach:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = poison_adj_dense[0][nid].nonzero()\n",
    "    # bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(poison_x[u].unsqueeze(0),poison_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 2813\n",
      "tensor([0.1483], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1675 2867\n",
      "tensor([0.1121], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1675 3347\n",
      "tensor([0.1121], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1421 3239\n",
      "tensor([0.1104], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1675 2867\n",
      "tensor([0.1121], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1675 3347\n",
      "tensor([0.1121], device='cuda:2', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "induct_adj_dense = to_dense_adj(induct_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = induct_x.shape[0] - poison_x.shape[0]\n",
    "for nid in atk_test_nodes:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "for nid in idx_test:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        # if(v in bkd_nids):\n",
    "        u = nid\n",
    "        print(nid,v)\n",
    "        print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_homophily(adj,x,device):\n",
    "    deg_vector = adj.sum(1)\n",
    "    deg_matrix = torch.diag(adj.sum(1)).to(device)\n",
    "    deg_matrix += torch.eye(len(adj)).to(device)\n",
    "    deg_inv_sqrt = deg_matrix.pow(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "    adj = torch.matmul(deg_inv_sqrt,adj)\n",
    "    adj = torch.matmul(adj,deg_inv_sqrt)\n",
    "    x_neg = adj @ x\n",
    "    node_sims = np.array([float(F.cosine_similarity(xn.unsqueeze(0),xx.unsqueeze(0))) for (xn,xx) in zip(x_neg,x)])   \n",
    "    # node_sims = np.array([torch.round(i,decimals=2) for i in node_sims])\n",
    "    # print(node_sims)\n",
    "    return node_sims\n",
    "bkd_graph_test_node_sims = calculate_graph_homophily(to_dense_adj(data.edge_index)[0].to(device),data.x.to(device),device)\n",
    "bkd_graph_train_node_sims = calculate_graph_homophily(to_dense_adj(poison_edge_index)[0].to(device),poison_x.to(device),device)\n",
    "clean_graph_node_sims = calculate_graph_homophily(to_dense_adj(induct_edge_index)[0].to(device),induct_x.to(device),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def to_percent(y,position):\n",
    "    return str(100*y)+\"%\"#这里可以用round（）函数设置取几位小数\n",
    "\n",
    "plt.hist(clean_graph_node_sims,bins=10,weights=[1./len(clean_graph_node_sims)]*len(clean_graph_node_sims),density=True, alpha=0.75, label='clean')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_test_node_sims),bins=20,weights=[1./len(bkd_graph_test_node_sims)]*len(bkd_graph_test_node_sims),density=True, alpha=0.75, label='poison')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "plt.hist(np.array(bkd_graph_train_node_sims),bins=20,weights=[1./len(bkd_graph_train_node_sims)]*len(bkd_graph_train_node_sims),density=True, alpha=0.75, label='attack')#这里weights是每一个数据的权重，这里设置是1，weights是和x等维的列表或者series\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "# plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.png\")\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_test_edge_index = to_dense_adj(data.edge_index)[0].nonzero()\n",
    "trigger_trigger_edge_sims = []\n",
    "trigger_trigger_edge_index = []\n",
    "\n",
    "trigger_target_edge_sims = []\n",
    "trigger_target_edge_index = []\n",
    "\n",
    "normal_normal_edge_sims = []\n",
    "normal_normal_edge_index = []\n",
    "\n",
    "trigger_normal_edge_sims = []\n",
    "trigger_normal_edge_index = []\n",
    "\n",
    "target_target_edge_sims = []\n",
    "target_target_edge_index = []\n",
    "for (u,v) in bkd_test_edge_index:\n",
    "    if ((v,u) in trigger_trigger_edge_index) or ((u,v) in trigger_trigger_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_target_edge_index) or ((u,v) in trigger_target_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in normal_normal_edge_index) or ((u,v) in normal_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_normal_edge_index) or ((u,v) in trigger_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in target_target_edge_index) or ((u,v) in target_target_edge_index):\n",
    "        continue\n",
    "    \n",
    "    if (u in bkd_nids) and (v in bkd_nids):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_trigger_edge_sims.append(edge_sims)\n",
    "        trigger_trigger_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_atk)) or ((v in bkd_nids) and (u in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_target_edge_sims.append(edge_sims)\n",
    "        trigger_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    if (u in idx_test) and (v in idx_test):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        normal_normal_edge_sims.append(edge_sims)\n",
    "        normal_normal_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_test)) or ((v in bkd_nids) and (u in idx_test)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_normal_edge_sims.append(edge_sims)\n",
    "        trigger_normal_edge_index.append((u,v))\n",
    "\n",
    "    if ((u in idx_atk) and (v in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        target_target_edge_sims.append(edge_sims)\n",
    "        target_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
